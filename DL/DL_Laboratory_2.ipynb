{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pu4MEuMzh67k"
   },
   "source": [
    "#Data manipulation in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aC0RTaHXizdT"
   },
   "source": [
    "To get started with deep learning, we will need to develop a few basic skills. All machine learning\n",
    "is concerned with extracting information from data. So we will begin by learning the practical\n",
    "skills for storing and manipulating data.\n",
    "\n",
    "To start, we introduce the\n",
    "$n$-dimensional array, which is also called the *tensor*. No matter which framework we use,\n",
    "its *tensor class* (`Tensor` in both PyTorch and TensorFlow) is similar to `numpy`'s `ndarray` with a few useful features. First, GPU is well-supported to accelerate the computation,\n",
    "whereas `numpy` only supports CPU computation. Second, the tensor class\n",
    "supports automatic differentiation.\n",
    "These properties make the tensor class suitable for deep learning.\n",
    "\n",
    "To start, we import `torch`. Note that though it's called PyTorch, we should\n",
    "import `torch`, instead of `pytorch`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "id": "JsKTdOxRkw-A"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLcSssask5uf"
   },
   "source": [
    "A tensor represents a (possibly multi-dimensional) array of numerical values.\n",
    "With one axis, a tensor is called a *vector*.\n",
    "With two axes, a tensor is called a *matrix*.\n",
    "With $k > 2$ axes, we drop the specialized names\n",
    "and just refer to the object as a $k$*th-order tensor*.\n",
    "\n",
    "PyTorch provides a variety of functions \n",
    "for creating new tensors \n",
    "prepopulated with values. \n",
    "For example, by invoking `arange(n)`,\n",
    "we can create a vector of evenly spaced values,\n",
    "starting at $0$ (included) \n",
    "and ending at `n` (not included).\n",
    "By default, the interval size is $1$.\n",
    "Unless otherwise specified, \n",
    "new tensors are stored in main memory \n",
    "and designated for CPU-based computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m6P1QeYnlQ-t",
    "outputId": "1d50c39b-6e0f-4878-e4f9-0ccb474495c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12, dtype=torch.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHtOMvH5laAd"
   },
   "source": [
    "We can access a tensor's *shape* (the length along each axis) by inspecting its `shape` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8hs0ukEIlhxr",
    "outputId": "5cf4b925-23d8-4e99-a35a-5ddd32060056"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1tkK84WlpzM"
   },
   "source": [
    "If we just want to know the total number of elements in a tensor,\n",
    "i.e., the product of all of the shape elements,\n",
    "we can inspect its size.\n",
    "Because we are dealing with a vector here,\n",
    "the single element of its `shape` is identical to its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n5u5VaVSlq2T",
    "outputId": "81bbd474-315a-442d-fae6-4dc5cd0459dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YeXiUfm0lwOr"
   },
   "source": [
    "To change the shape of a tensor without altering\n",
    "either the number of elements or their values,\n",
    "we can invoke the `reshape()` function.\n",
    "For example, we can transform our tensor, `x`,\n",
    "from a row vector with shape $(12,)$ to a matrix with shape $(3, 4)$.\n",
    "This new tensor contains the exact same values,\n",
    "but views them as a matrix organized as $3$ rows and $4$ columns.\n",
    "To reiterate, although the shape has changed,\n",
    "the elements have not.\n",
    "Note that the size is unaltered by reshaping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s0IOOcyomLsS",
    "outputId": "a7ff3c39-2f36-4ec4-89e2-1a79476c45b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = x.reshape(3, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVnkYR7VmTMM"
   },
   "source": [
    "Reshaping by manually specifying every dimension is unnecessary.\n",
    "If our target shape is a matrix with shape $(\\text{height, width})$,\n",
    "then, after we know the $\\text{width}$, the $\\text{height}$ is given implicitly.\n",
    "Why should we have to perform the division ourselves?\n",
    "In the example above, to get a matrix with $3$ rows,\n",
    "we specified both that it should have $3$ rows and $4$ columns.\n",
    "Fortunately, tensors can automatically work out one dimension given the rest.\n",
    "We invoke this capability by placing `-1` for the dimension\n",
    "that we would like tensors to automatically infer.\n",
    "In our case, instead of calling `x.reshape(3, 4)`,\n",
    "we could have equivalently called `x.reshape(-1, 4)` or `x.reshape(3, -1)`.\n",
    "\n",
    "Typically, we will want our matrices initialized\n",
    "either with zeros, ones, some other constants,\n",
    "or numbers randomly sampled from a specific distribution. We can create a tensor representing a tensor with all elements set to $0$ and a shape of $(2, 3, 4)$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FzRSAnPqm9wU",
    "outputId": "55b4b1ea-1a57-4f15-9c3a-7a615b340075"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((2, 3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZupzD-om_dB"
   },
   "source": [
    "Similarly, we can create tensors with each element set to $1$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3Ds6hqnnCd5",
    "outputId": "fad8b72b-58a0-4af0-d9c5-e44e73289e65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((2, 3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vqnYdY2nUlr"
   },
   "source": [
    "Often, we want to randomly sample the values\n",
    "for each element in a tensor from some probability distribution.\n",
    "For example, when we construct arrays to serve\n",
    "as parameters in a neural network, we will\n",
    "typically initialize their values randomly.\n",
    "The following code creates a tensor with shape $(3, 4)$.\n",
    "Each of its elements is randomly sampled\n",
    "from a standard Gaussian (normal) distribution\n",
    "with a mean of $0$ and a standard deviation of $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1CCEln8YngUc",
    "outputId": "ae3a2ab1-2e55-4ff6-be62-951b57c48221"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2759, -1.3638,  0.8270, -1.0733],\n",
       "        [ 0.4067, -0.9057,  0.3972,  0.4568],\n",
       "        [-1.4896, -1.8321, -0.3007,  0.7070]])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wX94B3JnmGN"
   },
   "source": [
    "We can also specify the exact values for each element in the desired tensor\n",
    "by supplying a Python list (or list of lists) containing the numerical values.\n",
    "Here, the outermost list corresponds to axis $0$, and the inner list to axis $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ym7rn87tnu3i",
    "outputId": "45b32749-645e-4505-fe84-5a9f197336bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1, 4, 3],\n",
       "        [1, 2, 3, 4],\n",
       "        [4, 3, 2, 1]])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRm4RLkYn4mI"
   },
   "source": [
    "Our interests are not limited to simply\n",
    "reading and writing data from/to arrays.\n",
    "We want to perform mathematical operations on those arrays.\n",
    "Some of the simplest and most useful operations\n",
    "are the *element-wise* operations.\n",
    "These apply a standard scalar operation\n",
    "to each element of an array.\n",
    "For functions that take two arrays as inputs,\n",
    "element-wise operations apply some standard binary operator\n",
    "on each pair of corresponding elements from the two arrays.\n",
    "We can create an element-wise function from any function\n",
    "that maps from a scalar to a scalar.\n",
    "\n",
    "In mathematical notation, we would denote such\n",
    "a *unary* scalar operator (taking one input)\n",
    "by the signature $f: \\mathbb{R} \\rightarrow \\mathbb{R}$.\n",
    "This just means that the function is mapping\n",
    "from any real number ($\\mathbb{R}$) to another.\n",
    "Likewise, we denote a *binary* scalar operator\n",
    "(taking two real inputs, and yielding one output)\n",
    "by the signature $f: \\mathbb{R}, \\mathbb{R} \\rightarrow \\mathbb{R}$.\n",
    "Given any two vectors $\\mathbf{u}$ and $\\mathbf{v}$ *of the same shape*,\n",
    "and a binary operator $f$, we can produce a vector\n",
    "$\\mathbf{c} = F(\\mathbf{u},\\mathbf{v})$\n",
    "by setting $c_i \\gets f(u_i, v_i)$ for all $i$,\n",
    "where $c_i, u_i$, and $v_i$ are the $i$th elements\n",
    "of vectors $\\mathbf{c}, \\mathbf{u}$, and $\\mathbf{v}$.\n",
    "Here, we produced the vector-valued operator\n",
    "$F: \\mathbb{R}^d, \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$\n",
    "by *lifting* the scalar function to an element-wise vector operation.\n",
    "\n",
    "The common standard arithmetic operators\n",
    "(`+`, `-`, `*`, `/`, and `**`)\n",
    "have all been *lifted* to element-wise operations\n",
    "for any identically-shaped tensors of arbitrary shape.\n",
    "We can call element-wise operations on any two tensors of the same shape.\n",
    "In the following example, we use commas to formulate a $5$-element tuple,\n",
    "where each element is the result of an element-wise operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RWTst0Hbor4B",
    "outputId": "00efe956-022a-45d6-f50a-9cebb59ca4f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.,  4.,  6., 10.]),\n",
       " tensor([-1.,  0.,  2.,  6.]),\n",
       " tensor([ 2.,  4.,  8., 16.]),\n",
       " tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n",
       " tensor([ 1.,  4., 16., 64.]))"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "x + y, x - y, x * y, x / y, x ** y  # The ** operator is exponentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSVpNpt7oysA"
   },
   "source": [
    "Many more operations can be applied element-wise, including unary operators like *exponentiation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uKLQOnRTo6BF",
    "outputId": "91285e00-048b-45b8-fa8f-f2a835f8fed0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNuTP5fipA6h"
   },
   "source": [
    "In addition to element-wise computations,\n",
    "we can also perform linear algebra operations,\n",
    "including vector dot products and matrix multiplication.\n",
    "\n",
    "We can also *concatenate* multiple tensors together, stacking them end-to-end to form a larger tensor.\n",
    "We just need to provide a list of tensors\n",
    "and tell the system along which axis to concatenate.\n",
    "The example below shows what happens when we concatenate\n",
    "two matrices along rows (axis $0$, the first element of the shape)\n",
    "vs. columns (axis $1$, the second element of the shape).\n",
    "We can see that the first output tensor's axis-$0$ length ($6$)\n",
    "is the sum of the two input tensors' axis-$0$ lengths ($3 + 3$);\n",
    "while the second output tensor's axis-$1$ length ($8$)\n",
    "is the sum of the two input tensors' axis-$1$ lengths ($4 + 4$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YbHlX4vmpx7e",
    "outputId": "2225c894-6765-4657-b40e-147b5ec5c271"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [ 2.,  1.,  4.,  3.],\n",
       "         [ 1.,  2.,  3.,  4.],\n",
       "         [ 4.,  3.,  2.,  1.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "torch.cat((X, Y), axis=0), torch.cat((X, Y), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GjsGQEIqPiW"
   },
   "source": [
    "Sometimes, we want to construct a binary tensor via *logical statements*.\n",
    "Take `X == Y` as an example.\n",
    "For each position, if `X` and `Y` are equal at that position,\n",
    "the corresponding entry in the new tensor takes a value of $1$,\n",
    "meaning that the logical statement `X == Y` is true at that position;\n",
    "otherwise, that position takes $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P-jcHl99qhoC",
    "outputId": "fe5a5f9d-75f0-42b2-932e-cb58301de3ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIDRqyE1qmK4"
   },
   "source": [
    "Summing all the elements in the tensor yields a tensor with only one element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f22ZOnxxqk9P",
    "outputId": "c0247148-3e4a-41c4-a592-57ea36d3cacd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66.)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxlhLQPjqxFV"
   },
   "source": [
    "We previously saw how to perform element-wise operations\n",
    "on two tensors of the same shape. Under certain conditions,\n",
    "even when shapes differ, we can still perform element-wise operations\n",
    "by using the *broadcasting mechanism*.\n",
    "This mechanism works in the following way:\n",
    "first, expand one or both arrays\n",
    "by copying elements appropriately\n",
    "so that, after this transformation,\n",
    "the two tensors have the same shape.\n",
    "Second, carry out the element-wise operations\n",
    "on the resulting arrays.\n",
    "\n",
    "In most cases, we broadcast along an axis where an array\n",
    "initially only has length $1$, such as in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBLUchG-rIoU",
    "outputId": "8a575767-9488-41d8-cdd0-31f6ac24a996"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([[0, 1]]))"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3).reshape((3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aM_IBfIDrNv7"
   },
   "source": [
    "Since `a` and `b` are $3\\times1$ and $1\\times2$ matrices, respectively,\n",
    "their shapes do not match up if we want to add them.\n",
    "We *broadcast* the entries of both matrices into a larger $3\\times2$ matrix as follows:\n",
    "for matrix `a` it replicates the columns,\n",
    "and for matrix `b` it replicates the rows,\n",
    "before adding up both element-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XK2lKVrjrzHF",
    "outputId": "b64a6c3e-6d19-45cf-c942-d091ad8414ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpNOEM5er3h0"
   },
   "source": [
    "Just as in any other Python array, elements in a tensor can be accessed by index.\n",
    "As in any Python array, the first element has index $0$\n",
    "and ranges are specified to include the first, but *before* the last element.\n",
    "As in standard Python lists, we can access elements\n",
    "according to their relative position to the end of the list\n",
    "by using negative indices.\n",
    "\n",
    "Thus, `[-1]` selects the last element and `[1:3]`\n",
    "selects the second and the third elements as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NgL-9toUsEa_",
    "outputId": "f7a4da75-4ebc-4012-b64f-e95031392407"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8.,  9., 10., 11.]),\n",
       " tensor([[ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]))"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-1], X[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlgEsxI1sJFy"
   },
   "source": [
    "Beyond reading, we can also write elements of a matrix by specifying indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sSGla6UusLaI",
    "outputId": "78330be1-aec7-4661-83bd-9f19f3834f2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  9.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1, 2] = 9\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FB25ZHensSMn"
   },
   "source": [
    "If we want to assign multiple elements the same value,\n",
    "we simply index all of them and then assign them the value.\n",
    "For instance, `[0:2, :]` accesses the first and second rows,\n",
    "where `:` takes all the elements along axis $1$ (column).\n",
    "While we discussed indexing for matrices,\n",
    "this obviously also works for vectors\n",
    "and for tensors of more than $2$ dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u4C41pCNshPu",
    "outputId": "baa36eea-4612-4f61-db65-82a171d7c74e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 12., 12., 12.],\n",
       "        [12., 12., 12., 12.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:2, :] = 12\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnagV6khsujy"
   },
   "source": [
    "Running operations can cause new memory to be\n",
    "allocated to host results. For example, if we write `Y = X + Y`,\n",
    "we will dereference the tensor that `Y` used to point to\n",
    "and instead point `Y` at the newly allocated memory location.\n",
    "In the following example, we demonstrate this with Python's `id()` function,\n",
    "which gives us the exact address of the referenced object in memory.\n",
    "After running `Y = Y + X`, we will find that `id(Y)` points to a different location.\n",
    "That is because Python first evaluates `Y + X`,\n",
    "allocating new memory for the result, and then makes `Y`\n",
    "point to this new location in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "txoztlMftAED",
    "outputId": "0205fde1-28c1-4c90-e2c1-fa90ca3d0ca4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(Y)\n",
    "Y = Y + X\n",
    "id(Y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbfFnJeytD0j"
   },
   "source": [
    "This might be undesirable for two reasons.\n",
    "First, we do not want to allocate memory unnecessarily all the time.\n",
    "In machine learning, we might have\n",
    "hundreds of megabytes of parameters\n",
    "and update all of them multiple times per second.\n",
    "Typically, we will want to perform these updates *in place*.\n",
    "Second, we might point at the same parameters from multiple variables.\n",
    "If we do not update in place, other references will still point to\n",
    "the old memory location, making it possible for parts of our code\n",
    "to inadvertently reference old parameters.\n",
    "\n",
    "Fortunately, performing in-place operations is easy.\n",
    "We can assign the result of an operation\n",
    "to a previously allocated array with slice notation,\n",
    "e.g., `Y[:] = <expression>`.\n",
    "To illustrate this concept, we first create a new matrix `Z`\n",
    "with the same shape as another `Y`,\n",
    "using `zeros_like()` to allocate a block of $0$ entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6fDBeQeqtmPf",
    "outputId": "ec8a576e-caf7-411f-9ba6-398d7fa3bb4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Z): 140255025699936\n",
      "id(Z): 140255025699936\n"
     ]
    }
   ],
   "source": [
    "Z = torch.zeros_like(Y)\n",
    "print('id(Z):', id(Z))\n",
    "Z[:] = X + Y\n",
    "print('id(Z):', id(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkZZvEYEttbj"
   },
   "source": [
    "If the value of `X` is not reused in subsequent computations,\n",
    "we can also use `X[:] = X + Y` or `X += Y`\n",
    "to reduce the memory overhead of the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZlLzcwGptyOY",
    "outputId": "bc9d864f-058d-4abb-9b7d-2a64d3c67fe9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(X)\n",
    "X += Y\n",
    "id(X) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oqx4niaNt1Po"
   },
   "source": [
    "Converting to a `numpy` tensor (`ndarray`), or vice versa, is easy.\n",
    "The PyTorch `Tensor` and `numpy` `ndarray` will share their underlying memory\n",
    "locations, and changing one through an in-place operation will also\n",
    "change the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LQ_Prko7vKSv",
    "outputId": "5c31aa97-1e00-4692-be3f-9a2733dca18d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, torch.Tensor)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = X.numpy()\n",
    "B = torch.from_numpy(A)\n",
    "type(A), type(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSZhm_KvyWuG"
   },
   "source": [
    "To convert a size-$1$ tensor to a Python scalar,\n",
    "we can invoke the `item` function or Python's built-in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fR90WPuzyemQ",
    "outputId": "43fc83e6-eec6-4ace-90f5-bc03ff1c2468"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.5000]), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([3.5])\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DwdgYacW15w"
   },
   "source": [
    "#Linear algebra in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEWmPNwP5NSg"
   },
   "source": [
    "Now that we know how to store and manipulate data, we will introduce the basic mathematical objects, arithmetic,\n",
    "and operations in *linear algebra*,\n",
    "expressing them through mathematical notation\n",
    "and the corresponding implementation in code.\n",
    "\n",
    "Formally, we call values consisting\n",
    "of just one numerical quantity *scalars*.\n",
    "\n",
    "A scalar is represented by a tensor with just one element. Next, we instantiate two scalars\n",
    "and perform some familiar arithmetic operations with them,\n",
    "namely addition, multiplication, division, and exponentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "imAyxsif59SZ",
    "outputId": "a8eb2e40-2154-4510-9506-6770036fffaa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "x + y, x * y, x / y, x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kok_nELG6EXM"
   },
   "source": [
    "We can think of a *vector* as simply a list of scalar values. We call these values the *elements* (*entries* or *components*) of the vector.\n",
    "\n",
    "We work with vectors via one-dimensional tensors.\n",
    "In general, tensors can have arbitrary lengths,\n",
    "subject to the memory limits of our machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0IBG0Eb_6XO-",
    "outputId": "e90271d3-df54-444c-a224-3665098c84f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkXBiE9u6fGe"
   },
   "source": [
    "We can access any element by indexing into the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AbFlcQiH6ea6",
    "outputId": "2d929d5e-b5fa-43c0-aac5-d9bc40d5eeac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mx_CBqTo6wuN"
   },
   "source": [
    "The length of a vector is commonly called the *dimension* of the vector.\n",
    "\n",
    "As with an ordinary Python array,\n",
    "we can access the length of a tensor by calling Python's built-in `len()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8yz4A4m460QG",
    "outputId": "25040d6a-a2eb-4007-c23a-e7a13743d836"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CT10nFXS64z2"
   },
   "source": [
    "When a tensor represents a vector (with precisely one axis),\n",
    "we can also access its length via the `.shape` attribute.\n",
    "The shape is a tuple that lists the length (dimensionality)\n",
    "along each axis of the tensor. For tensors with just one axis, the shape has just one element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0MJq_-WQ6-XT",
    "outputId": "0f543078-cc8f-430c-fc9d-86c4cf818979"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytDovjBK7EsB"
   },
   "source": [
    "Note that the word \"dimension\" tends to get overloaded\n",
    "in these contexts and this tends to be confusing.\n",
    "To clarify, we use the dimensionality of a *vector* or an *axis*\n",
    "to refer to its length, i.e., the number of elements of a vector or an axis.\n",
    "However, we use the dimensionality of a tensor\n",
    "to refer to the number of axes that a tensor has.\n",
    "In this sense, the dimensionality of some axis of a tensor\n",
    "will be the length of that axis.\n",
    "\n",
    "Just as vectors generalize scalars from order zero to order one,\n",
    "matrices generalize vectors from order one to order two.\n",
    "\n",
    "We can create an $m \\times n$ matrix by specifying a shape with two components, $m$ and $n$, when calling any of the functions for instantiating a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YmMvPf-N9Gii",
    "outputId": "04efc157-4aea-4f4b-9dca-7f981b678779"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15],\n",
       "        [16, 17, 18, 19]])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20).reshape(5, 4)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhdzOE029iUk"
   },
   "source": [
    "Sometimes, we want to flip the axes of a matrix.\n",
    "When we exchange a matrix's rows and columns,\n",
    "the result is the *transpose* of the matrix.\n",
    "\n",
    "We can access a matrix's transpose in code by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ewRo8EV091fM",
    "outputId": "1034ce40-1e20-4478-a907-5b4068ecbf07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  4,  8, 12, 16],\n",
       "        [ 1,  5,  9, 13, 17],\n",
       "        [ 2,  6, 10, 14, 18],\n",
       "        [ 3,  7, 11, 15, 19]])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bs8sXwoa9-EM"
   },
   "source": [
    "As a special type of square matrix, a *symmetric matrix* is equal to its transpose. Here, we define a symmetric matrix `B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zNQo_UHC-Gu8",
    "outputId": "65f78de2-88a5-4a5f-9d04-adf20b870ea3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [2, 0, 4],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNxjYk32-LOj"
   },
   "source": [
    "Now we compare `B` with its transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ebgEYhC899eT",
    "outputId": "1cfa1c6f-8dec-47ff-b7e2-4b770e0fae51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B == B.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGsqkmfS-bdl"
   },
   "source": [
    "Although the default orientation of a single vector is a column vector, in a matrix that represents a tabular dataset, it is more\n",
    "conventional to treat each data example as a row vector in the matrix. For example, along the\n",
    "outermost axis of a tensor, we can access or enumerate mini-batches of data examples, or just data\n",
    "examples, if no mini-batch exists.\n",
    "\n",
    "Just as vectors generalize scalars, and matrices generalize vectors, we can build data structures with even more axes. *Tensors* give us a generic way of describing $n$-dimensional arrays with an arbitrary number of axes. Vectors, for example, are first-order tensors, and matrices are second-order tensors.\n",
    "\n",
    "Tensors will become more important when we start working with images,\n",
    " which are represented as $n$-dimensional arrays with $3$ axes corresponding to the height, width, and a *channel* axis for stacking the color channels (red, green, and blue). For now, we will skip over higher-order tensors and focus on the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0AfmL_g--_cr",
    "outputId": "07bc0b2a-4212-4a78-9368-b3cb7570def9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yIMAZ4p_F4I"
   },
   "source": [
    "Scalars, vectors, matrices, and tensors of an arbitrary number of axes have some nice properties that are often useful.\n",
    "For example, we might have noticed\n",
    "from the definition of an element-wise operation\n",
    "that any element-wise unary operation does not change the shape of its operand.\n",
    "Similarly, given any two tensors with the same shape,\n",
    "the result of any binary element-wise operation\n",
    "will be a tensor of that same shape.\n",
    "For example, adding two matrices of the same shape\n",
    "performs element-wise addition over these two matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yLwP4_5I_YCw",
    "outputId": "e134400b-2bd0-4d1d-9b87-639c85a7c553"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[ 0.,  2.,  4.,  6.],\n",
       "         [ 8., 10., 12., 14.],\n",
       "         [16., 18., 20., 22.],\n",
       "         [24., 26., 28., 30.],\n",
       "         [32., 34., 36., 38.]]))"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "B = A.clone()  # Assign a copy of `A` to `B` by allocating new memory\n",
    "A, A + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJQ9SRcK_d1Q"
   },
   "source": [
    "Specifically, element-wise multiplication of two matrices is called their *Hadamard product*, and is denoted by $\\odot$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TyWt5vCx_nS-",
    "outputId": "5749a283-20e0-434d-9ec9-4966767c1293"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   1.,   4.,   9.],\n",
       "        [ 16.,  25.,  36.,  49.],\n",
       "        [ 64.,  81., 100., 121.],\n",
       "        [144., 169., 196., 225.],\n",
       "        [256., 289., 324., 361.]])"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaa8H3l1_sQV"
   },
   "source": [
    "Multiplying or adding a tensor by a scalar also does not change the shape of the tensor,\n",
    "where each element of the operand tensor will be added or multiplied by the scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pKy8lcXp_wi3",
    "outputId": "de4eac52-b2d2-49f3-c1d7-768a2afb0f32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2,  3,  4,  5],\n",
       "          [ 6,  7,  8,  9],\n",
       "          [10, 11, 12, 13]],\n",
       " \n",
       "         [[14, 15, 16, 17],\n",
       "          [18, 19, 20, 21],\n",
       "          [22, 23, 24, 25]]]),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "a + X, (a * X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZr7mPRu_2oT"
   },
   "source": [
    "One useful operation that we can perform with arbitrary tensors\n",
    "is to calculate the sum of their elements.\n",
    "We can just call the `sum()` function for calculating the sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hW8wsfVgADoy",
    "outputId": "08e7d397-0d77-4b51-af0c-104957b3464c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor(6.))"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4, dtype=torch.float32)\n",
    "x, x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKJLKPYeAIdo"
   },
   "source": [
    "We can express sums over the elements of tensors of arbitrary shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3rgX6-FYAKhL",
    "outputId": "a66f0ab1-cbb0-41dd-b5b7-affa63e47c0a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4]), tensor(190.))"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, A.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxUC5QCCAPcv"
   },
   "source": [
    "By default, invoking the function for calculating the sum\n",
    "*reduces* a tensor along all its axes to a scalar.\n",
    "We can also specify the axes along which the tensor is reduced via summation.\n",
    "Take matrices as an example.\n",
    "To reduce the row dimension (axis $0$) by summing up elements of all the rows,\n",
    "we specify `axis=0` when invoking the function.\n",
    "Since the input matrix reduces along axis $0$ to generate the output vector,\n",
    "the dimension of axis $0$ of the input is lost in the output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BvcU1QHOAehP",
    "outputId": "b3920c5c-25b3-4384-9199-d75cea0005fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([40., 45., 50., 55.]), torch.Size([4]))"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis0 = A.sum(axis=0)\n",
    "A_sum_axis0, A_sum_axis0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81v5za8zAjjK"
   },
   "source": [
    "Specifying `axis=1` will reduce the column dimension (axis $1$) by summing up elements of all the columns. Thus, the dimension of axis $1$ of the input is lost in the output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J66XP9zKAppS",
    "outputId": "6935f0b0-b658-46b9-e666-d0cfda5d9012"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis1 = A.sum(axis=1)\n",
    "A_sum_axis1, A_sum_axis1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3ylmXz_AtLE"
   },
   "source": [
    "Reducing a matrix along both rows and columns via summation\n",
    "is equivalent to summing up all the elements of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7jIn8iLAtsJ",
    "outputId": "627e3842-b7c1-4111-fa4a-96e2065ae1f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(190.)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=[0, 1])  # Same as `A.sum()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sXvfP0aAzPT"
   },
   "source": [
    "A related quantity is the *mean*, which is also called the *average*. We calculate the mean by dividing the sum by the total number of elements.\n",
    "In code, we could just call the function for calculating the mean\n",
    "on tensors of arbitrary shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OAUElqBzA9J4",
    "outputId": "cf0f9388-4f2b-440c-fd1f-0dfe63ff52ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(9.5000), tensor(9.5000))"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(), A.sum() / A.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULNfc6cDA_kE"
   },
   "source": [
    "Likewise, the function for calculating the mean can also reduce a tensor along the specified axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jSBTa_tZBDoZ",
    "outputId": "71b9e96d-f23d-4951-f27e-f0286bb1e922"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8.,  9., 10., 11.]), tensor([ 8.,  9., 10., 11.]))"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(axis=0), A.sum(axis=0) / A.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YGh-rNaFpIt"
   },
   "source": [
    "However, sometimes it can be useful to keep the number of axes unchanged when invoking the\n",
    "function for calculating the sum or mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V8SYXYUnFsJK",
    "outputId": "1af0bf2a-04cd-4380-d3c1-01558f2d7153"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.],\n",
       "        [22.],\n",
       "        [38.],\n",
       "        [54.],\n",
       "        [70.]])"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMrE9JeMFyFP"
   },
   "source": [
    "For instance, since `sum_A` still keeps its two axes after summing each row, we can divide `A` by `sum_A` with broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bpl4JAEWF2f_",
    "outputId": "6947558a-28b1-44dc-ada6-57113b1e8609"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n",
       "        [0.1818, 0.2273, 0.2727, 0.3182],\n",
       "        [0.2105, 0.2368, 0.2632, 0.2895],\n",
       "        [0.2222, 0.2407, 0.2593, 0.2778],\n",
       "        [0.2286, 0.2429, 0.2571, 0.2714]])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A / sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNRd6ZnbF9AZ"
   },
   "source": [
    "If we want to calculate the cumulative sum of elements of `A` along some axis, say `axis=0` (row by row),\n",
    "we can call the `cumsum()` function. This function will not reduce the input tensor along any axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bBq887PdGF8Y",
    "outputId": "2c1726b9-8c15-4cc4-e0be-7a1c08bf5594"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  6.,  8., 10.],\n",
       "        [12., 15., 18., 21.],\n",
       "        [24., 28., 32., 36.],\n",
       "        [40., 45., 50., 55.]])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.cumsum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x49kC_8sGPbT"
   },
   "source": [
    "So far, we have only performed element-wise operations, sums, and averages. However, one of the most fundamental operations is the dot product.\n",
    "Given two vectors, their *dot product* is a sum over the products of the elements at the same position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ae4YEwSnIrs5",
    "outputId": "e50f964a-7bf5-439f-f87a-73caaa8b130e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor([1., 2., 3., 4.]), tensor(20.))"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.arange(1, 5, dtype = torch.float32)\n",
    "x, y, torch.dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgzdtZyFJjtt"
   },
   "source": [
    "Note that we can express the dot product of two vectors equivalently by performing an element-wise multiplication and then a sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HaosPcxDKOEa",
    "outputId": "3d9dac6d-4f47-4447-ec4f-bdb96029fc84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20.)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1BtxxIvKmRT"
   },
   "source": [
    "Dot products are useful in a wide range of contexts.\n",
    "For example, given some set of values,\n",
    "denoted by a vector $\\mathbf{x}  \\in \\mathbb{R}^d$,\n",
    "and a set of weights denoted by $\\mathbf{w} \\in \\mathbb{R}^d$,\n",
    "the weighted sum of the values in $\\mathbf{x}$\n",
    "according to the weights $\\mathbf{w}$\n",
    "could be expressed as the dot product $\\mathbf{x}^\\top \\mathbf{w}=\\mathbf{w}^\\top \\mathbf{x}$.\n",
    "When the weights are non-negative\n",
    "and sum to one (i.e., $\\sum_{i=1}^{d} {w_i} = 1$),\n",
    "the dot product expresses a *weighted average*.\n",
    "After normalizing two vectors to have the unit length,\n",
    "the dot products express the cosine of the angle between them.\n",
    "\n",
    "Now that we know how to calculate dot products,\n",
    "we can begin to understand *matrix-vector products*.\n",
    "Let matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$\n",
    "and the vector $\\mathbf{x} \\in \\mathbb{R}^n$. We start off by visualizing the matrix $\\mathbf{A}$ in terms of its row vectors\n",
    "\n",
    "$$\\mathbf{A}=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_m \\\\\n",
    "\\end{bmatrix},$$\n",
    "\n",
    "where each $\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^n$\n",
    "is a row vector representing the $i$th row of the matrix $\\mathbf{A}$.\n",
    "\n",
    "The matrix-vector product $\\mathbf{A}\\mathbf{x}$\n",
    "is simply a column vector of length $m$,\n",
    "whose $i$th element is the dot product $\\mathbf{a}^\\top_i \\mathbf{x}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}\\mathbf{x}\n",
    "= \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_m \\\\\n",
    "\\end{bmatrix}\\mathbf{x}\n",
    "= \\begin{bmatrix}\n",
    " \\mathbf{a}^\\top_{1} \\mathbf{x}  \\\\\n",
    " \\mathbf{a}^\\top_{2} \\mathbf{x} \\\\\n",
    "\\vdots\\\\\n",
    " \\mathbf{a}^\\top_{m} \\mathbf{x}\\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "We can think of multiplication by a matrix $\\mathbf{A}\\in \\mathbb{R}^{m \\times n}$\n",
    "as a transformation that projects vectors\n",
    "from $\\mathbb{R}^{n}$ to $\\mathbb{R}^{m}$.\n",
    "These transformations turn out to be very useful.\n",
    "For example, we can represent rotations\n",
    "as multiplications by a square matrix.\n",
    "\n",
    "Expressing matrix-vector products in code with tensors, we use\n",
    "the `mv()` function. When we call `torch.mv(A, x)` with a matrix\n",
    "`A` and a vector `x`, the matrix-vector product is performed.\n",
    "Note that the column dimension of `A` (its length along axis $1$)\n",
    "must be the same as the dimension of `x` (its length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KpEcrFrhMYce",
    "outputId": "bb489608-da28-4df4-aa33-ea9c9a8bbd75"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, x.shape, torch.mv(A, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8A2WD2CN6A8"
   },
   "source": [
    "Now that we understand dot products and matrix-vector products, *matrix-matrix multiplication* should be straightforward.\n",
    "\n",
    "Assume that we have two matrices $\\mathbf{A} \\in \\mathbb{R}^{n \\times k}$ and $\\mathbf{B} \\in \\mathbb{R}^{k \\times m}$:\n",
    "\n",
    "$$\\mathbf{A}=\\begin{bmatrix}\n",
    " a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n",
    " a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " a_{n1} & a_{n2} & \\cdots & a_{nk} \\\\\n",
    "\\end{bmatrix},\\quad\n",
    "\\mathbf{B}=\\begin{bmatrix}\n",
    " b_{11} & b_{12} & \\cdots & b_{1m} \\\\\n",
    " b_{21} & b_{22} & \\cdots & b_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " b_{k1} & b_{k2} & \\cdots & b_{km} \\\\\n",
    "\\end{bmatrix}.$$\n",
    "\n",
    "\n",
    "Denote by $\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^k$\n",
    "the row vector representing the $i$th row of the matrix $\\mathbf{A}$,\n",
    "and let $\\mathbf{b}_{j} \\in \\mathbb{R}^k$\n",
    "be the column vector from the $j$th column of the matrix $\\mathbf{B}$.\n",
    "To produce the matrix product $\\mathbf{C} = \\mathbf{A}\\mathbf{B}$, it is easiest to think of $\\mathbf{A}$ in terms of its row vectors and of $\\mathbf{B}$ in terms of its column vectors:\n",
    "\n",
    "$$\\mathbf{A}=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_n \\\\\n",
    "\\end{bmatrix},\n",
    "\\quad \\mathbf{B}=\\begin{bmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "Then, the matrix product $\\mathbf{C} \\in \\mathbb{R}^{n \\times m}$ is produced by simply computing each element $c_{ij}$ as the dot product $\\mathbf{a}^\\top_i \\mathbf{b}_j$:\n",
    "\n",
    "$$\\mathbf{C} = \\mathbf{AB} = \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_n \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\mathbf{b}_1 & \\mathbf{a}^\\top_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^\\top_{1} \\mathbf{b}_m \\\\\n",
    " \\mathbf{a}^\\top_{2}\\mathbf{b}_1 & \\mathbf{a}^\\top_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^\\top_{2} \\mathbf{b}_m \\\\\n",
    " \\vdots & \\vdots & \\ddots &\\vdots\\\\\n",
    "\\mathbf{a}^\\top_{n} \\mathbf{b}_1 & \\mathbf{a}^\\top_{n}\\mathbf{b}_2& \\cdots& \\mathbf{a}^\\top_{n} \\mathbf{b}_m\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "We can think of the matrix-matrix multiplication $\\mathbf{AB}$ as simply performing $m$ matrix-vector products and stitching the results together to form an $n \\times m$ matrix.\n",
    "In the following, we perform matrix multiplication on `A` and `B`.\n",
    "Here,`A` is a matrix with $5$ rows and $4$ columns,\n",
    "and `B` is a matrix with $4$ rows and $3$ columns.\n",
    "After multiplication, we obtain a matrix with $5$ rows and $3$ columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ApeICSE3PDyO",
    "outputId": "1d480597-3991-4ad8-aaa4-2dc1a1620379"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  6.,  6.],\n",
       "        [22., 22., 22.],\n",
       "        [38., 38., 38.],\n",
       "        [54., 54., 54.],\n",
       "        [70., 70., 70.]])"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.ones(4, 3)\n",
    "torch.mm(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLpaOG1GPIDi"
   },
   "source": [
    "Matrix-matrix multiplication can be simply called *matrix multiplication*, and should not be confused\n",
    "with the Hadamard product. Matrix multiplication can also be performed using the `@` operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2DERfgf7Pd2u",
    "outputId": "6e3bee18-5fde-4769-f256-504d2fb4910f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 6.,  6.,  6.],\n",
       "         [22., 22., 22.],\n",
       "         [38., 38., 38.],\n",
       "         [54., 54., 54.],\n",
       "         [70., 70., 70.]]),\n",
       " tensor([ 14.,  38.,  62.,  86., 110.]))"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A @ B, A @ x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FN7g3xKtQNYJ"
   },
   "source": [
    "Some of the most useful operators in linear algebra are *norms*.\n",
    "Informally, the norm of a vector tells us how *big* a vector is.\n",
    "The notion of *size* under consideration here\n",
    "concerns not dimensionality,\n",
    "but rather the magnitude of the components.\n",
    "\n",
    "The familiar Euclidean distance is a norm:\n",
    "specifically, it is the $\\ell_2$ norm.\n",
    "Suppose that the elements in the $n$-dimensional vector\n",
    "$\\mathbf{x}$ are $x_1, \\ldots, x_n$.\n",
    "\n",
    "The $\\ell_2$ *norm* of $\\mathbf{x}$ is the square root of the sum of the squares of the vector elements:\n",
    "\n",
    "$$\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2},$$\n",
    "\n",
    "In code, we can calculate the $\\ell_2$ norm of a vector as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FTTGEUNvQ2zw",
    "outputId": "0fb9438b-e47e-40f1-851e-7cccd1c9aeee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.norm(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4--zktZeQ9gu"
   },
   "source": [
    "We will also frequently encounter the $\\ell_1$ *norm*, which is expressed as the sum of the absolute values of the vector elements:\n",
    "\n",
    "$$\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|.$$\n",
    "\n",
    "\n",
    "As compared with the $\\ell_2$ norm,\n",
    "it is less influenced by outliers.\n",
    "To calculate the $\\ell_1$ norm, we compose\n",
    "the absolute value function with a sum over the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4G8nThBOROMQ",
    "outputId": "e6062e69-dede-40cb-9c5f-93a7825870cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(u).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjvhbf0FRUIi"
   },
   "source": [
    "Analogous to $\\ell_2$ norms of vectors, the *Frobenius norm* of a matrix $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$ is the square root of the sum of the squares of the matrix elements:\n",
    "\n",
    "$$\\|\\mathbf{X}\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}.$$\n",
    "\n",
    "The Frobenius norm satisfies all the properties of vector norms.\n",
    "It behaves as if it were an $\\ell_2$ norm of a matrix-shaped vector.\n",
    "Invoking the `norm()` function will calculate the Frobenius norm of a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KTLRAJvKRl--",
    "outputId": "75ca2463-95e5-4565-ae94-293c8be5d4d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torch.ones((4, 9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65J-KObrSEp-"
   },
   "source": [
    "# Automatic differentiation in PyTorch\n",
    "\n",
    "Differentiation is a crucial step in nearly all deep learning optimization algorithms.\n",
    "While the calculations for taking these derivatives are straightforward,\n",
    "requiring only some basic calculus,\n",
    "for complex models, working out the updates by hand\n",
    "can be very complicated (and often error-prone).\n",
    "\n",
    "Deep learning frameworks speed up this work\n",
    "by automatically calculating derivatives, i.e., *automatic differentiation*.\n",
    "In practice,\n",
    "based on our designed model,\n",
    "the system builds a *computational graph*,\n",
    "tracking which data combined through\n",
    "which operations to produce the output.\n",
    "Automatic differentiation enables the system to subsequently backpropagate gradients.\n",
    "Here, *backpropagate* simply means to trace through the computational graph,\n",
    "filling in the partial derivatives with respect to each parameter.\n",
    "\n",
    "As an example, assume that we are interested\n",
    "in differentiating the function\n",
    "$y = 2\\mathbf{x}^{\\top}\\mathbf{x}$\n",
    "with respect to the column vector $\\mathbf{x}$.\n",
    "To start, we create the variable `x` and assign it an initial value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xRJHoViMSwh7",
    "outputId": "02d9a58e-241e-4eca-e5a2-b3ab6a563eb7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gjSeXPfS0iN"
   },
   "source": [
    "Before we even calculate the gradient\n",
    "of $y$ with respect to $\\mathbf{x}$,\n",
    "we will need a place to store it.\n",
    "It is important that we do not allocate new memory\n",
    "every time we take a derivative with respect to a parameter,\n",
    "because we will often update the same parameters\n",
    "thousands or millions of times,\n",
    "and could quickly run out of memory.\n",
    "Note that a gradient of a scalar-valued function\n",
    "with respect to a vector $\\mathbf{x}$\n",
    "is itself vector-valued and has the same shape as $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "id": "LZkDGzNDTDh2"
   },
   "outputs": [],
   "source": [
    "x.requires_grad_(True)  # Same as `x = torch.arange(4.0, requires_grad=True)`\n",
    "x.grad  # The default value is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0cY1GpdTJPR"
   },
   "source": [
    "Now, let us calculate $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UjJSQftkTDo1",
    "outputId": "1644bfd0-0906-4ef2-9569-f85a27772349"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzmdFCn8TPwz"
   },
   "source": [
    "Since `x` is a vector of length $4$,\n",
    "a dot product of `x` and `x` is performed,\n",
    "yielding the scalar output that we assign to `y`.\n",
    "Next, we can automatically calculate the gradient of `y`\n",
    "with respect to each component of `x`\n",
    "by calling the `backward()` function for backpropagation, and then printing the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ll0Y1UQMTh91",
    "outputId": "6398fa65-863b-4d47-9e74-8ef51366cef3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_zpPbYrTxy3"
   },
   "source": [
    "The gradient of the function $y = 2\\mathbf{x}^{\\top}\\mathbf{x}$\n",
    "with respect to $\\mathbf{x}$ should be $4\\mathbf{x}$.\n",
    "Let us quickly verify that our desired gradient was calculated correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RyvzeJk4T4Rt",
    "outputId": "df78e30d-7bb2-4cbd-a2ed-3c359d69ca49"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGMpwTevUSzY"
   },
   "source": [
    "Now, let us calculate another function of `x`, the sum of its elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RbE4cqLfUV01",
    "outputId": "606444b5-e1c7-48f4-abcc-83a2ac91ce4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch accumulates the gradient by default, we need to clear the previous\n",
    "# values\n",
    "print(x)\n",
    "x.grad.zero_()\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3oRsFW3UsaI"
   },
   "source": [
    "Sometimes, we wish to move some calculations outside of the recorded computational graph.\n",
    "For example, say that `y` was calculated as a function of `x`,\n",
    "and that subsequently `z` was calculated as a function of both `y` and `x`.\n",
    "Now, imagine that we wanted to calculate\n",
    "the gradient of `z` with respect to `x`,\n",
    "but wanted for some reason to treat `y` as a constant,\n",
    "and only take into account the role\n",
    "that `x` played after `y` was calculated.\n",
    "\n",
    "Here, we can *detach* `y` to return a new variable `u`\n",
    "that has the same value as `y`, but discards any information\n",
    "about how `y` was computed in the computational graph.\n",
    "In other words, the gradient will not flow backwards through `u` to `x`.\n",
    "Thus, the following backpropagation function computes\n",
    "the partial derivative of `z = u * x` with respect to `x`, while treating `u` as a constant,\n",
    "instead of the partial derivative of `z = x * x * x` with respect to `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AVm7jg9zU9bP",
    "outputId": "ff864cd9-5808-409d-da1b-d6a39fbaa6cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLAjuM98VEiS"
   },
   "source": [
    "Since the computation of `y` was recorded,\n",
    "we can subsequently invoke backpropagation on `y` to get the derivative of `y = x * x` with respect to `x`, which is `2 * x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "js9kjkj_VHlm",
    "outputId": "37f2122f-c6a6-474e-f01a-fd9cdc62c7a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRT04M_gVWg4"
   },
   "source": [
    "One benefit of using automatic differentiation\n",
    "is that, even if building the computational graph of a function\n",
    "required passing through Python control flow (e.g., conditionals, loops, and arbitrary function calls), we can still calculate the gradient of the resulting variable.\n",
    "In the following, note that\n",
    "the number of iterations of the `while` loop\n",
    "and the evaluation of the `if` statement\n",
    "both depend on the value of the input `a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "id": "e8XJw2qQVmuc"
   },
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O52kBI7OVrfw"
   },
   "source": [
    "Let us compute the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "id": "i8JHKiG5Vr-W"
   },
   "outputs": [],
   "source": [
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXdXij_UVwT9"
   },
   "source": [
    "We can now analyze the `f` function defined above.\n",
    "Note that it is piecewise linear in its input `a`.\n",
    "In other words, for any `a`, there exists some constant scalar `k`\n",
    "such that `f(a) = k * a`, where the value of `k` depends on the input `a`.\n",
    "Consequently `d / a` allows us to verify that the gradient is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yn2KRQl8V6yN",
    "outputId": "925794df-31a1-41a9-9563-cdae7a79c4ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad == d / a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2]), torch.Size([3, 4]), torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ex.1\n",
    "\n",
    "dim1 = torch.randn(2)\n",
    "dim2 = torch.randn(3, 4)\n",
    "dim3 = torch.randn(2, 3, 4)\n",
    "dim1.shape, dim2.shape, dim3.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.2026e+04, -4.5400e-05, -3.3546e-04], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Ex.2\n",
    "\n",
    "x = torch.tensor([-10, 10, 8], dtype=torch.float64)\n",
    "x.requires_grad_(True)\n",
    "y = 1/1+torch.exp(-x)\n",
    "y.sum().backward()\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1,  4],\n",
       "         [ 2,  5],\n",
       "         [-3, 10]]),\n",
       " tensor([[10, 10],\n",
       "         [10, 10],\n",
       "         [10, 10]]))"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ex.3\n",
    "\n",
    "A = torch.tensor([[1, 2, -3], [4, 5, 10]])\n",
    "B = torch.tensor([[10, 11, -2], [13, -3, 8]])\n",
    "\n",
    "A * B\n",
    "B[:] = 10\n",
    "A.T, B.T\n",
    "\n",
    "# torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(26., dtype=torch.float64), tensor(20.4939, dtype=torch.float64))"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ex.4\n",
    "\n",
    "v = torch.tensor([-4, 2, 20], dtype=torch.float64) # float\n",
    "\n",
    "l1 = torch.abs(v).sum()\n",
    "l2 = torch.norm(v)\n",
    "\n",
    "l1, l2"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
