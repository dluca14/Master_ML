{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jhyhiSJVqWn"
   },
   "source": [
    "#Recurrent neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suoLpSoTs4Vq"
   },
   "source": [
    "First, we add the necessary imports and set a constant seed for the random number generator in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wBjOO9kPDJMT"
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import collections\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "\n",
    "torch.manual_seed(42);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPKDgFrzth4c"
   },
   "source": [
    "Text is one of the most popular examples of sequence data. For example, an article can be simply viewed as a sequence of words, or even a sequence of characters. The common preprocessing steps for text, usually, are:\n",
    "\n",
    "1. Load text as strings into memory.\n",
    "1. Split strings into tokens (e.g., words and characters).\n",
    "1. Build a table of vocabulary to map the split tokens to numerical indices.\n",
    "1. Convert text into sequences of numerical indices so they can be manipulated by models easily.\n",
    "\n",
    "To get started we load text from H. G. Wells' [*The Time Machine*](http://www.gutenberg.org/ebooks/35). This is a fairly small corpus of just over $30000$ words, but, for the purpose of what we want to illustrate, this is just fine. More realistic document collections contain many billions of words. The `read_time_machine()` function reads the dataset into a list of text lines, where each line is a string. For simplicity, here we ignore punctuation and capitalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "B1Q50eDmDD_p"
   },
   "outputs": [],
   "source": [
    "def download(url, cache_dir=os.path.join('..', 'data')):\n",
    "    \"\"\"Download a file, return the local filename.\"\"\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
    "    if os.path.exists(fname):\n",
    "        with open(fname, 'rb') as f:\n",
    "            while True:\n",
    "                data = f.read(1048576)\n",
    "                if not data:\n",
    "                    break\n",
    "        return fname\n",
    "    print(f'Downloading {fname} from {url}...')\n",
    "    r = requests.get(url, stream=True, verify=True)\n",
    "    with open(fname, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    return fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6PMDPiHuCc-h",
    "outputId": "f20fbc24-27bf-4bcf-cb8e-00395e9df5b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ..\\data\\timemachine.txt from http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt...\n",
      "# text lines: 3221\n",
      "the time machine by h g wells\n",
      "twinkled and his usually pale face was flushed and animated the\n"
     ]
    }
   ],
   "source": [
    "def read_time_machine():\n",
    "    \"\"\"Load the time machine dataset into a list of text lines.\"\"\"\n",
    "    with open(download('http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
    "\n",
    "lines = read_time_machine()\n",
    "print(f'# text lines: {len(lines)}')\n",
    "print(lines[0])\n",
    "print(lines[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5T--FND7uYhr"
   },
   "source": [
    "The following `tokenize()` function takes a list (`lines`) as the input, where each element is a text sequence (e.g., a text line). Each text sequence is split into a list of tokens. A *token* is the basic unit in text. In the end, a list of token lists are returned, where each token is a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9NHosKb7CW_c",
    "outputId": "eab38285-3f87-4ba4-bf9c-2dbd84517cb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['i']\n",
      "[]\n",
      "[]\n",
      "['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n",
      "['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n",
      "['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(lines, token='word'):\n",
    "    \"\"\"Split text lines into word or character tokens.\"\"\"\n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('ERROR: unknown token type: ' + token)\n",
    "\n",
    "tokens = tokenize(lines)\n",
    "for i in range(11):\n",
    "    print(tokens[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kp9vPYJ5upVt"
   },
   "source": [
    "The string type of the token is inconvenient to be used by models, which take numerical inputs. Now let us build a dictionary, often called *vocabulary* as well, to map string tokens into numerical indices starting from $0$. To do so, we first count the unique tokens in all the documents from the training set, namely a *corpus*, and then assign a numerical index to each unique token according to its frequency. Rarely appeared tokens are often removed to reduce the complexity. Any token that does not exist in the corpus or has been removed is mapped into a special unknown token “&lt;unk&gt;”. We optionally add a list of reserved tokens, such as “&lt;pad&gt;” for padding, “&lt;bos&gt;” to represent the beginning of a sequence, and “&lt;eos&gt;” for the end of a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vwnOjpzzCNjy"
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        # Sort according to frequencies\n",
    "        counter = count_corpus(tokens)\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                   reverse=True)\n",
    "        # The index for the unknown token is 0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # Index for the unknown token\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):  # Token frequencies\n",
    "        return self._token_freqs\n",
    "\n",
    "def count_corpus(tokens):\n",
    "    \"\"\"Count token frequencies.\"\"\"\n",
    "    # Here `tokens` is a 1D list or 2D list\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # Flatten a list of token lists into a list of tokens\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNTQL-FkvUPy"
   },
   "source": [
    "We construct a vocabulary using the `time machine` dataset as the corpus. Then, we print the first few frequent tokens with their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eSfvoEGkvTqK",
    "outputId": "b40ecff2-2902-4ee5-a402-f4506f74e937"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<unk>', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(tokens)\n",
    "print(list(vocab.token_to_idx.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfbL1e35vm3n"
   },
   "source": [
    "Now, we can convert each text line into a list of numerical indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yWJfWY2Nvp-z",
    "outputId": "3b674830-0c0d-4d8a-b1e0-e5d3da570ecb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "indices: [1, 19, 50, 40, 2183, 2184, 400]\n",
      "words: ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n",
      "indices: [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]\n"
     ]
    }
   ],
   "source": [
    "for i in [0, 10]:\n",
    "    print('words:', tokens[i])\n",
    "    print('indices:', vocab[tokens[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zvg1oSBrvunW"
   },
   "source": [
    "Using the above functions, we package everything into the `load_corpus_time_machine()` function, which returns `corpus`, a list of token indices, and `vocab`, the vocabulary of the time machine corpus.\n",
    "The modifications we did here are:\n",
    "(i) we tokenize text into characters, not words, to simplify the later training;\n",
    "(ii) `corpus` is a single list, not a list of token lists, since each text line in the `time machine` dataset is not necessarily a sentence or a paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99Br8poOCEUr",
    "outputId": "3ee42ffe-acf3-4b6a-80b5-c0fb9a864f26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<unk>', 0), (' ', 1), ('e', 2), ('t', 3), ('a', 4), ('i', 5), ('n', 6), ('o', 7), ('s', 8), ('h', 9), ('r', 10), ('d', 11), ('l', 12), ('m', 13), ('u', 14), ('c', 15), ('f', 16), ('w', 17), ('g', 18), ('y', 19), ('p', 20), ('b', 21), ('v', 22), ('k', 23), ('x', 24), ('z', 25), ('j', 26), ('q', 27)]\n"
     ]
    }
   ],
   "source": [
    "def load_corpus_time_machine(max_tokens=-1):\n",
    "    \"\"\"Return token indices and the vocabulary of the time machine dataset.\"\"\"\n",
    "    lines = read_time_machine()\n",
    "    tokens = tokenize(lines, 'char')\n",
    "    vocab = Vocab(tokens)\n",
    "    # Since each text line in the time machine dataset is not necessarily a\n",
    "    # sentence or a paragraph, flatten all the text lines into a single list\n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    return corpus, vocab\n",
    "\n",
    "corpus, vocab = load_corpus_time_machine()\n",
    "len(corpus), len(vocab)\n",
    "print(list(vocab.token_to_idx.items())[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVE_0JDIwbZ1"
   },
   "source": [
    "We can ensure that the subsequences from two adjacent mini-batches during iteration are adjacent on the original sequence. This strategy preserves the order of split subsequences when iterating over mini-batches, hence is called *sequential partitioning*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SrdiwSTBsxl"
   },
   "outputs": [],
   "source": [
    "def seq_data_iter_sequential(corpus, batch_size, num_steps):\n",
    "    \"\"\"Generate a mini-batch of subsequences using sequential partitioning.\"\"\"\n",
    "    # Start with a random offset to partition a sequence\n",
    "    offset = random.randint(0, num_steps)\n",
    "    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
    "    Xs = torch.tensor(corpus[offset: offset + num_tokens])\n",
    "    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n",
    "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
    "    num_batches = Xs.shape[1] // num_steps\n",
    "    for i in range(0, num_steps * num_batches, num_steps):\n",
    "        X = Xs[:, i: i + num_steps]\n",
    "        Y = Ys[:, i: i + num_steps]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-v28nktDxC9c"
   },
   "source": [
    "Let us manually generate a sequence from $0$ to $34$. We assume that the batch size and number of time steps are $2$ and $5$, respectively. This means that we can generate $\\lfloor (35 - 1) / 5 \\rfloor= 6$ feature-label subsequence pairs. With a mini-batch size of $2$, we only get $3$ mini-batches. \n",
    "\n",
    "Let us print features `X` and labels `Y` for each mini-batch of subsequences read by sequential partitioning. Note that the subsequences from two adjacent mini-batches during iteration are indeed adjacent in the original sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pKUPBqXyxQZT",
    "outputId": "9bcd7d7e-c14a-4fc5-a871-c50554638419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  tensor([[ 2,  3,  4,  5,  6],\n",
      "        [18, 19, 20, 21, 22]]) \n",
      "Y: tensor([[ 3,  4,  5,  6,  7],\n",
      "        [19, 20, 21, 22, 23]])\n",
      "X:  tensor([[ 7,  8,  9, 10, 11],\n",
      "        [23, 24, 25, 26, 27]]) \n",
      "Y: tensor([[ 8,  9, 10, 11, 12],\n",
      "        [24, 25, 26, 27, 28]])\n",
      "X:  tensor([[12, 13, 14, 15, 16],\n",
      "        [28, 29, 30, 31, 32]]) \n",
      "Y: tensor([[13, 14, 15, 16, 17],\n",
      "        [29, 30, 31, 32, 33]])\n"
     ]
    }
   ],
   "source": [
    "my_seq = list(range(35))\n",
    "for X, Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5):\n",
    "    print('X: ', X, '\\nY:', Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsvQaQ7bx3FB"
   },
   "source": [
    "Now, we wrap the above two sampling functions to a class, so that we can use it as a data iterator later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5pXs-IoBHtw"
   },
   "outputs": [],
   "source": [
    "class SeqDataLoader:\n",
    "    \"\"\"An iterator to load sequence data.\"\"\"\n",
    "    def __init__(self, batch_size, num_steps, max_tokens):\n",
    "        self.corpus, self.vocab = load_corpus_time_machine(max_tokens)\n",
    "        self.batch_size, self.num_steps = batch_size, num_steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        return seq_data_iter_sequential(self.corpus, self.batch_size, self.num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nes6WA9yJf8"
   },
   "source": [
    "Lastly, we define a function `load_data_time_machine()` that returns both the data iterator and the vocabulary, so we can use it similarly as other functions with the `load_data` prefix, such as `load_data_fashion_mnist()` defined in *Laboratory 3*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxWxgOcS_auI"
   },
   "outputs": [],
   "source": [
    "def load_data_time_machine(batch_size, num_steps, max_tokens=10000):\n",
    "    \"\"\"Return the iterator and the vocabulary of the time machine dataset.\"\"\"\n",
    "    data_iter = SeqDataLoader(\n",
    "        batch_size, num_steps, max_tokens)\n",
    "    return data_iter, data_iter.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "743mlIXwzCyj"
   },
   "source": [
    "In order to implement a language model with RNNs, we use functions provided by the high-level APIs of PyTorch. We begin by reading the `time machine` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmWtkkDzE5im"
   },
   "outputs": [],
   "source": [
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdUS7dvKzdN8"
   },
   "source": [
    "High-level APIs provide implementations of recurrent neural networks. We construct the recurrent neural network layer `rnn_layer` with a single hidden layer and $256$ hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6OUsw2KE8_o"
   },
   "outputs": [],
   "source": [
    "num_hiddens = 256\n",
    "rnn_layer = nn.RNN(len(vocab), num_hiddens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHKr4eqQz5ET"
   },
   "source": [
    "We use a tensor to initialize the hidden state, whose shape is\n",
    "(number of hidden layers, batch size, number of hidden units)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fMga5UNiFFZZ",
    "outputId": "611145e9-704c-41a9-a453-185808088938"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 256])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = torch.zeros((1, batch_size, num_hiddens))\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmQH4slC0ALm"
   },
   "source": [
    "With a hidden state and an input, we can compute the output with the updated hidden state. It should be emphasized that the \"output\" (`Y`) of `rnn_layer` does *not* involve computation of output layers: it refers to the hidden state at *each* time step, and they can be used as the input to the subsequent output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kaj6poo5FHbX",
    "outputId": "e3e6ecc8-c466-44d5-c5fe-327b64a7fef8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([35, 32, 256]), torch.Size([1, 32, 256]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(size=(num_steps, batch_size, len(vocab)))\n",
    "Y, state_new = rnn_layer(X, state)\n",
    "Y.shape, state_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIfiIdBeItzh"
   },
   "source": [
    "Recall that each token is represented as a numerical index in `train_iter`. Feeding these indices directly to a neural network might make it hard to learn. We often represent each token as a more expressive feature vector. The easiest representation is called *one-hot encoding*.\n",
    "\n",
    "In a nutshell, we map each index to a different unit vector: assume that the number of different tokens in the vocabulary is $N$ (`len(vocab)`) and the token indices range from $0$ to $N-1$. If the index of a token is the integer $i$, then we create a vector of all $0$s with a length of $N$ and set the element at position $i$ to $1$. This vector is the one-hot vector of the original token. The one-hot vectors with indices $0$ and $2$ are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xwERALMJJUN",
    "outputId": "19869bfa-488b-4d59-ce01-61ba0fcf6e52"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor([0, 2]), len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrM4MzXrJM96"
   },
   "source": [
    "The shape of the mini-batch that we sample each time is (batch size, number of time steps). The `F.one_hot()` function transforms such a mini-batch into a three-dimensional tensor, where the last dimension equals to the vocabulary size (`len(vocab)`). We often transpose the input, so that we will obtain an output of shape (number of time steps, batch size, vocabulary size). This will allow us to more conveniently loop through the outermost dimension for updating hidden states of a mini-batch, time step by time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TR5Lhs-YJv4V",
    "outputId": "1b622d02-64f3-40b9-ffbb-0ad8232ccec6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 28])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(10).reshape((2, 5))\n",
    "F.one_hot(X.T, 28).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYM1tweo0OP8"
   },
   "source": [
    "We define an `RNNModel` class for a complete RNN model. Note that `rnn_layer` only contains the hidden recurrent layers, so we need to create a separate output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_cPsMX3tFKI8"
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"The RNN model.\"\"\"\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size\n",
    "        self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
    "        X = X.to(torch.float32)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        # The fully connected layer will first change the shape of `Y` to\n",
    "        # (`num_steps` * `batch_size`, `num_hiddens`). Its output shape is\n",
    "        # (`num_steps` * `batch_size`, `vocab_size`).\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            # `nn.GRU` takes a tensor as hidden state\n",
    "            return  torch.zeros((self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens),\n",
    "                                 device=device)\n",
    "        else:\n",
    "            # `nn.LSTM` takes a tuple of hidden states\n",
    "            return (torch.zeros((\n",
    "                self.rnn.num_layers,\n",
    "                batch_size, self.num_hiddens), device=device),\n",
    "                    torch.zeros((\n",
    "                        self.rnn.num_layers,\n",
    "                        batch_size, self.num_hiddens), device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhL7VPtM0uPN"
   },
   "source": [
    "Let us first define the prediction function to generate new characters following\n",
    "the user-provided `prefix`, which is a string containing several characters.  When looping through these beginning characters in `prefix`, we keep passing the hidden state to the next time step without generating any output. This is called the *warm-up* period, during which the model updates itself (e.g., update the hidden state), but does not make predictions. After the warm-up period, the hidden state is generally better than its initialized value at the beginning. So we generate the predicted characters and emit them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qhqJVv_XFlIM"
   },
   "outputs": [],
   "source": [
    "def predict(prefix, num_preds, net, vocab, device):\n",
    "    \"\"\"Generate new characters following the `prefix`.\"\"\"\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
    "    for y in prefix[1:]:  # Warm-up period\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds):  # Predict `num_preds` steps\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0txXSfB1Rhq"
   },
   "source": [
    "Now we can test the `predict()` function. We specify the prefix as `time traveller` and have it generate $10$ additional characters. Given that we have not trained the network yet, i.e., the model has random weights, it will generate nonsensical predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwJpmF1GFebv"
   },
   "outputs": [],
   "source": [
    "def try_gpu(i=0):\n",
    "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "eapyU5gTFOcx",
    "outputId": "c76a43f5-d229-4714-a960-4317cd8c75d1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'time travellervv<unk>vvvvvvv'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = try_gpu()\n",
    "net = RNNModel(rnn_layer, vocab_size=len(vocab))\n",
    "net = net.to(device)\n",
    "predict('time traveller', 10, net, vocab, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUYVRxzg10HO"
   },
   "source": [
    "Below we define a function to *clip the gradients* of a model that is constructed by the high-level APIs. Also, note that we compute the gradient norm over all the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7kQbdoDkF78-"
   },
   "outputs": [],
   "source": [
    "def grad_clipping(net, theta):\n",
    "    \"\"\"Clip the gradient.\"\"\"\n",
    "    params = [p for p in net.parameters() if p.requires_grad]\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8EUuKWc2KBr"
   },
   "source": [
    "Before training the model, let us define a function to train the model in one epoch. It differs from how we train the model of *Laboratory 3* in three places:\n",
    "\n",
    "1. Different types of recurrent layers will result in differences in the initialization of hidden states.\n",
    "1. We clip the gradients before updating the model parameters. This ensures that the model does not diverge, even when gradients blow up at some point during the training process.\n",
    "1. We use perplexity to evaluate the model. As discussed in the course, this ensures that sequences of different lengths are comparable.\n",
    "\n",
    "Specifically, when sequential partitioning is used, we initialize the hidden state only at the beginning of each epoch. Since the $i$th subsequence example in the next mini-batch is adjacent to the current $i$th subsequence example, the hidden state at the end of the current mini-batch will be used to initialize the hidden state at the beginning of the next mini-batch. In this way, historical information of the sequence stored in the hidden state might flow over adjacent subsequences within an epoch. However, the computation of the hidden state at any point depends on all the previous mini-batches in the same epoch, which complicates the gradient computation. To reduce computational cost, we detach the gradient before processing any mini-batch, so that the gradient computation of the hidden state is always limited to the time steps in one mini-batch. \n",
    "\n",
    "Same as the `train_epoch()` function in *Laboratory 3*, `optimizer` is a built-in optimization function in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EmBMWd6vF4VV"
   },
   "outputs": [],
   "source": [
    "def train_epoch(net, train_iter, loss, optimizer, device):\n",
    "    \"\"\"Train a net within one epoch.\"\"\"\n",
    "    state = None\n",
    "    # Sum of training loss, no. of tokens\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    for X, Y in train_iter:\n",
    "        if state is None:\n",
    "            # Initialize `state` when it is the first iteration\n",
    "            state = net.begin_state(batch_size=X.shape[0], device=device)\n",
    "        else:\n",
    "            if not isinstance(state, tuple):\n",
    "                # `state` is a tensor for `nn.GRU`\n",
    "                state.detach_()\n",
    "            else:\n",
    "                # `state` is a tuple of tensors for `nn.LSTM`\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "        y = Y.T.reshape(-1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_hat, state = net(X, state)\n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        grad_clipping(net, 1)\n",
    "        optimizer.step()\n",
    "        total_loss += float(l * y.numel())\n",
    "        total_tokens += y.numel()\n",
    "    return math.exp(total_loss / total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8X8TY2qj32lD"
   },
   "source": [
    "The training function supports an RNN model implemented using high-level APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vr59uClMG-bo"
   },
   "outputs": [],
   "source": [
    "def train(net, train_iter, vocab, lr, num_epochs, device):\n",
    "    \"\"\"Train a model.\"\"\"\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    perplexities = []\n",
    "    # Initialize\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr)\n",
    "    # Train and predict\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl = train_epoch(\n",
    "            net, train_iter, loss, optimizer, device)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(predict('time traveller', 50, net, vocab, device))\n",
    "            perplexities.append(ppl)\n",
    "    print(f'perplexity {ppl:.1f}, device {str(device)}')\n",
    "    print(predict('time traveller', 50, net, vocab, device))\n",
    "    print(predict('traveller', 50, net, vocab, device))\n",
    "\n",
    "    return perplexities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJQZD20NOdck"
   },
   "source": [
    "Now we can train the RNN model by calling the `train()` function. Since we only use $10000$ tokens in the dataset, the model needs more epochs to converge better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D6RQ706HFVlc",
    "outputId": "93e85cb4-9aa2-42a8-8127-2db0c80e06e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time traveller the the the the the the the the the the the the t\n",
      "time traveller and the the the the the the the the the the the t\n",
      "time traveller the the the the the the the the the the the the t\n",
      "time traveller the this the thice some that sime time sion so di\n",
      "time travellere med an ans ano he mant of the thave and have the\n",
      "time travellerthithe monght rousthe tree the the thatour and the\n",
      "time travellerickthe thith sion so deand in the reatlist ou the \n",
      "time traveller surded this timension of his there that in a dint\n",
      "time traveller curee and the enot of camd an aile hathist is tha\n",
      "time traveller pat en sthere wrea touredinst in time tore ard th\n",
      "time travelleris tha fexthestithe wspectofte sime tiok an thes a\n",
      "time travellerit to ge there wral thenting thanethe thile creerp\n",
      "time traveller suthed this soree blate move cong there was avire\n",
      "time traveller the cince arealdiche time traveller the limather \n",
      "time travellerit to ag her aby cand the tome trovelle back the f\n",
      "time travelleris the praven bug ton tis the than s onit te ickte\n",
      "time travelleris tlatwe hty us a gomee in tored andide st thone \n",
      "time traveller s that is yot one angot waitat at wsak as whe hea\n",
      "time travellerit would in wall gat exalliven tand to cante tedth\n",
      "time travellerit wouldife ss the furred snow in taly une have av\n",
      "time traveller scinely d areche warl exper mine along the mithe \n",
      "time travellerit s against reason ta s and bry toukentime pori w\n",
      "time traveller the atmott un thas is sore al and how in por ans \n",
      "time travellerit s against reason said filby bet you warnermilbs\n",
      "time travellerit s against reason sasd fol any of thattirit and \n",
      "time travellerit o lars ho knot or call one s mannes time than l\n",
      "time traveller thr a done there was al mounaid annat whisediman \n",
      "time traveller hald iner abmelthene of the gereot surelouth stir\n",
      "time traveller came back and for d as on mald any hirguithest th\n",
      "time traveller friceeded heve inger at wass the tyon sammot mean\n",
      "time traveller arter thot hestan e of upe bove frow of storis le\n",
      "time traveller hal sanding than that no alconly on mone ably tim\n",
      "time travellerit s against reason said fily of courdean the thir\n",
      "time traveller proceeded the provitabyoc thackness cin fored rea\n",
      "time traveller cuthe wsye and the time traveller cute is ale hav\n",
      "time traveller smiled if on he pack to beendeno so a cime ta oor\n",
      "time traveller firectinnt tt rover tas er ans spaceenol treco hi\n",
      "time traveller after the pauserequired for thecpres have done so\n",
      "time traveller proceeded an move spait and there was thatluxurio\n",
      "time traveller hamd bnca fore wilnshis one thing diencrof pastio\n",
      "time travellerit on phe dtagnever he herenth af beas the faul th\n",
      "time traveller hald aner for shakend indst ond why candotsed ino\n",
      "time traveller proceeded hry tal gork tower suthentine or hre fo\n",
      "time traveller held in his hand was a glitteringmetallic framewo\n",
      "time traveller but now anouthe mode masterations of space but yo\n",
      "time traveller held in his hand was f anithat y anof thing is th\n",
      "time traveller came that is all right said the psychologist thou\n",
      "time traveller hole about in time for instance if a d haure the \n",
      "time travelleryou can maving fourthid and in tinet it tas o jurk\n",
      "time traveller but now you begin to seethe object of my investig\n",
      "perplexity 1.3, device cpu\n",
      "time traveller but now you begin to seethe object of my investig\n",
      "traveller but now you begin to seethe object of my investig\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 500, 1\n",
    "perplexities = train(net, train_iter, vocab, lr, num_epochs, device) #1 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4B8zHv6PMbw"
   },
   "source": [
    "Finally, we plot the perplexities obtained during training, using the `plot_perplexity()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YvS_979fHXRO"
   },
   "outputs": [],
   "source": [
    "def plot_perplexity(perplexities):\n",
    "    epochs = range(10, len(perplexities * 10) + 1, 10)\n",
    "    plt.plot(epochs, perplexities, 'b', label='Train perplexity') \n",
    "    plt.title('Training perplexity') \n",
    "    plt.xlabel('Epochs') \n",
    "    plt.ylabel('Perplexity') \n",
    "    plt.legend()  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "WeOdstSoIfgG",
    "outputId": "7f347484-83af-4bd9-9230-537f79fc7e87"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZwU1bn/8c/DADPAsCjixiKgiDKIg44CLglqIkSI6xD1J1wUjUs0Kr+436i4JSYm0RBNXC4Bc6NGBRFiEgUXXIKKQ9wFg0RQVFZl3+G5f5waGIZhphmmu6a7vu/Xq15dXV1d9ZxmeKrq1KlzzN0REZHkaBB3ACIikllK/CIiCaPELyKSMEr8IiIJo8QvIpIwSvwiIgmjxC9Zw8z+YWZD63rd+szMxpjZ7XWwnZVm1rkuYpLs1zDuACS3mdnKCm+bAuuATdH7i9z9kVS35e7fS8e6SeDuheXzZjYGmOfuP40vIomTEr+kVaWEMwe4wN2fr7yemTV0942ZjC1uSSyz1A+q6pFYmFlfM5tnZtea2XxgtJntZmbPmNkiM/smmm9X4TtTzOyCaP5cM3vNzH4VrfupmX2vlut2MrNXzGyFmT1vZveZ2Z9riPsGM1tsZnPM7JwKn+dH+/nMzBaY2f1m1qSaMle7vSr2P9DM3jGzpWY21cx6RMvPjMrVInr/PTObb2ZtovduZgeY2YXAOcA1UfXPX83sajMbV2k/I83st6n+e0p2UeKXOO0N7A7sB1xI+HscHb3vAKwB7q3m+72Aj4E9gF8Co8zMarHuo8A0oDUwAhiSQtx7AG2BocCDZtY1+uxO4ECgGDggWuemaspc0/a2MLOewB+Bi6JYHwAmmlm+uz8OTAVGmllrYBTh6mpRxW24+4PAI8Av3b3Q3b8P/Bnob2atov00BM4C/lTD7yBZSolf4rQZuNnd17n7Gndf4u7j3H21u68A7gC+Xc3357r7Q+6+CXgY2AfYa2fWNbMOwBHATe6+3t1fAyamEPuNUdwvA38DfhAdSC4Ehrv711EZfkZIolWWubrtVbHPC4EH3P1Nd9/k7g8T7pn0jj6/FDgemAL81d2fSaEcuPtXwCvAoGhRf2Cxu09P5fuSfZT4JU6L3H1t+Rsza2pmD5jZXDNbTkhGrcwsbwffn18+4+6ro9nCnVx3X+DrCssAPq8h7m/cfVWF93Oj7bQh3MCeHlXFLAWejZaX26bMNWyvsv2An5RvO9p++/J13X0p8CTQHfh1DWWo7GFgcDQ/GPjfnfy+ZBElfolT5a5hfwJ0BXq5ewvgW9HyHVXf1IWvgN3NrGmFZe1r+M5uZtaswvsOwJfAYkL1VJG7t4qmlhVvcLN9mavbXmWfA3dU2HYrd2/q7o8BmFkxMAx4DBhZTfxVxfA00MPMugMDCdVBkqOU+KU+aU5InEvNbHfg5nTv0N3nAmXACDNrbGZ9gO+n8NVbovWPJSTKJ919M/AQcLeZ7QlgZm3NrF9ttlfFOg8BF5tZLwuamdkAM2tuZgWEuvobgPOAtmb2ox3sawGwTZv+6CpkLNH9Dnf/LIWYJUsp8Ut9cg/QhHDm/AahmiQTzgH6AEuA24HHCXXnOzIf+IZwVv4IcLG7z4w+uxb4BHgjqq56nnAVU53qtreFu5cBPyTc8P4m2s+50cc/Bz539z+4+zpCdc3tZtaliv2NArpF1UVPV1j+MHAIqubJeaaBWES2ZWaPAzPdfbsrDjPrC/zZ3dtt98Xa7atOt7eLsXQAZgJ7u/vyuOOR9NEZvySemR1hZvubWQMz6w+cQqjzTgwzawD8f+AvSvq5T0/uioR29E8R2sbPAy5x97fjDSlzohvLCwitifrHHI5kgKp6REQSRlU9IiIJkxVVPXvssYd37Ngx7jBERLLK9OnTF7t7m8rLsyLxd+zYkbKysrjDEBHJKmY2t6rlquoREUkYJX4RkYRR4hcRSZisqOMXkfTZsGED8+bNY+3ayp2GSrYoKCigXbt2NGrUKKX1lfhFEm7evHk0b96cjh07suNxbKS+cneWLFnCvHnz6NSpU0rfUVWPSMKtXbuW1q1bK+lnKTOjdevWO3XFpsQvIkr6WW5n//1yOvE/+ijcf3/cUYiI1C85nfjHjYN77ok7ChGpzpIlSyguLqa4uJi9996btm3bbnm/fv36ar9bVlbG5ZdfnqFIqzdnzhy6d+9eq+9OnDiRO++8E4Cnn36ajz76qC5D205O39wtKoIJE2DtWigoiDsaEalK69ateeeddwAYMWIEhYWFXHXVVVs+37hxIw0bVp2qSkpKKCkpyUicNcWyK04++WROPvlkICT+gQMH0q1btzrfT7mcPuMvKoJNm+Djj+OORER2xrnnnsvFF19Mr169uOaaa5g2bRp9+vShZ8+eHHXUUXwc/aeeMmUKAwcOBMJBY9iwYfTt25fOnTszcmTVww4XFhYyfPhwioqKOOGEE1i0aBEAs2fPpn///hx++OEce+yxzJw5s8pYRowYwZAhQ+jTpw9dunThoYce2m4fmzZt4uqrr+aII46gR48ePPDAAwDcfffdDBs2DID333+f7t27s3r1asaMGcNll13G1KlTmThxIldffTXFxcXMnj2bww47bMt2Z82atc372sr5M36ADz+EQw+NNxaRbHDllRCdfNeZ4uLaVbnOmzePqVOnkpeXx/Lly3n11Vdp2LAhzz//PDfccAPjxo3b7jszZ87kpZdeYsWKFXTt2pVLLrlku7btq1atoqSkhLvvvptbb72VW265hXvvvZcLL7yQ+++/ny5duvDmm2/yox/9iBdffHG7WEaMGMF7773HG2+8wapVq+jZsycDBgzYZh+jRo2iZcuWvPXWW6xbt46jjz6aE088kSuuuIK+ffsyfvx47rjjDh544AGaNm265XtHHXUUJ598MgMHDqS0tBSAli1b8s4771BcXMzo0aM577zzdv7HrCSnE/+BB0LDhiHxi0h2GTRoEHl5eQAsW7aMoUOHMmvWLMyMDRs2VPmdAQMGkJ+fT35+PnvuuScLFiygXbttR7Vs0KABZ555JgCDBw/m9NNPZ+XKlUydOpVBgwZtWW/duq3DLleMBeCUU06hSZMmNGnShOOOO45p06ZRXFy85fNJkybx3nvvMXbs2C3xz5o1i06dOjFmzBh69OjBRRddxNFHH13j73DBBRcwevRofvOb3/D4448zbdq0Gr9Tk5xO/I0bQ5cuSvwiqapPjSGaNWu2Zf7GG2/kuOOOY/z48cyZM4e+fftW+Z38/Pwt83l5eWzcuLHG/ZgZmzdvplWrVlvuNVQXS/l3qnvv7vzud7+jX79+221r1qxZFBYW8uWXX9YYG8AZZ5zBLbfcwvHHH8/hhx9O69atU/pedXK6jh+ge3clfpFst2zZMtq2bQvAmDFjdmlbmzdv3nIm/uijj3LMMcfQokULOnXqxJNPPgmExP3uu+/ucBsTJkxg7dq1LFmyhClTpnDEEUds83m/fv34wx/+sOXK5N///jerVq1i2bJlXH755bzyyissWbJkSxwVNW/enBUrVmx5X1BQQL9+/bjkkkvqpJoHEpD4i4pg9mxYvTruSESktq655hquv/56evbsmdJZfHWaNWvGtGnT6N69Oy+++CI33XQTAI888gijRo3i0EMPpaioiAkTJuxwGz169OC4446jd+/e3Hjjjey7777bfH7BBRfQrVs3DjvsMLp3785FF13Exo0bGT58OJdeeikHHnggo0aN4rrrrmPhwoXbfPess87irrvuomfPnsyePRuAc845hwYNGnDiiSfuUtnLZcWYuyUlJV7bgVjGjoVBg2D6dKiDm+EiOWfGjBkcfPDBcYeRMYWFhaxcubLW36+qyWm6/epXv2LZsmXcdtttO1ynqn9HM5vu7tu1d83pOn7YtmWPEr+IZJvTTjuN2bNnb2lhVBdyPvEfcAA0aqR6fhEJduVsH8IZfyaNHz++zreZ83X8jRpB165K/CLVyYYqX9mxnf33y/nED6G6R4lfpGoFBQUsWbJEyT9LlffHX7AT/dLkfFUPhCadjz8OK1dCYWHc0YjUL+3atWPevHlbui6Q7FM+AleqEpH4y2/wzpgBlZrbiiReo0aNUh65SXJDYqp6QNU9IiKQkMS///6Qn6/ELyICCUn8eXlw0EFK/CIikJDED2rZIyJSLjGJv3t3+OwzWL487khEROKVmMRffoM3zUNZiojUe4lL/KruEZGkS1viN7M/mtlCM/ugwrLdzWyymc2KXndL1/4r69QJmjRR4hcRSecZ/xigf6Vl1wEvuHsX4IXofUY0aAAHH6zELyKStsTv7q8AX1dafArwcDT/MHBquvZfFbXsERHJfB3/Xu7+VTQ/H9hrRyua2YVmVmZmZXXVh0hREXzxBSxdWiebExHJSrHd3PXQFeAOuwN09wfdvcTdS9q0aVMn++zePbzqrF9EkizTiX+Bme0DEL0urGH9OqWWPSIimU/8E4Gh0fxQYMejGadBhw7QrJkSv4gkWzqbcz4GvA50NbN5ZnY+cCfwXTObBXwnep8xDRpAt25K/CKSbGnrj9/dz97BRyeka5+pKCqCZ5+NMwIRkXgl5sndckVFMH8+LFkSdyQiIvFIZOIHVfeISHIlLvGrSaeIJF3iEn+7dtCihRK/iCRX4hK/mVr2iEiyJS7xQ6jn/+AD8B0+NywikrsSmfhLSmDxYvj447gjERHJvEQm/pNOCq9/+1u8cYiIxCGRib9Dh9C6R4lfRJIokYkfYMAAePVVWLYs7khERDIr0Yl/40aYPDnuSEREMiuxib9PH2jVStU9IpI8iU38DRtCv37w97/D5s1xRyMikjmJTfwQqnsWLoTp0+OOREQkcxKd+Pv3D0/yqrpHRJIk0Ym/TRvo1UuJX0SSJdGJH0J1T1lZ6KNfRCQJlPgHhFeNyiUiSZH4xF9cDPvuq+oeEUmOxCd+s9B3z6RJsGFD3NGIiKRf4hM/hOqe5cvhtdfijkREJP2U+IHvfAcaN1Z1j4gkgxI/UFgI3/62Er+IJIMSf2TAAJg5E/7zn7gjERFJLyX+SHmzTp31i0iuU+KPHHAAHHigEr+I5D4l/goGDIApU2DVqrgjERFJHyX+Ck47Ddatg9Gj445ERCR9lPgrOOaY0Lrntttg5cq4oxERSQ8l/grM4M47Qx/9d98ddzQiIukRS+I3s+Fm9qGZfWBmj5lZQRxxVKV3bzj1VLjrLli8OO5oRETqXsYTv5m1BS4HSty9O5AHnJXpOKpzxx3hBu/PfhZ3JCIidS+uqp6GQBMzawg0Bb6MKY4qdesGQ4fCfffBZ5/FHY2ISN3KeOJ39y+AXwGfAV8By9x9UqbjqMmIEaHO/+ab445ERKRuxVHVsxtwCtAJ2BdoZmaDq1jvQjMrM7OyRYsWZTpMOnSASy+FP/0JPvww47sXEUmbOKp6vgN86u6L3H0D8BRwVOWV3P1Bdy9x95I2bdpkPEiAG24IHbj993/HsnsRkbSII/F/BvQ2s6ZmZsAJwIwY4qhR69Zw9dUwYQK8/nrc0YiI1I046vjfBMYC/wLej2J4MNNxpOrKK2GvveC668A97mhERHZdLK163P1mdz/I3bu7+xB3XxdHHKkoLIQbb4RXXoExY+KORkRk1+nJ3RT88IfwrW/BsGFw66068xeR7KbEn4LGjcNg7EOGhOadQ4bA2rVxRyUiUjsN4w4gW+Tnw8MPQ9eu8NOfwpw5MH48xNTgSESk1nTGvxPMQtPOJ56A6dOhVy/46KO4oxIR2TlK/LUwaBC8/DKsXg1HHRUe8lq/Pu6oRERSo8RfS0ceCdOmQefOoV+f9u3D1YD69hGR+k6Jfxd06ABlZfDss6E75zvvhE6d4JRT4LnnYPPmuCMUEdleSonfzFqnO5Bs1aAB9OsXnu799FO4/np44w3o3z8cAERE6ptUz/jfMLMnzeykqJsFqUKHDnD77fD55zB8ODzzTGj9IyJSn6Sa+A8kdKswBJhlZj8zswPTF1Z2a9wYfvzjMD9uXLyxiIhUllLi92Cyu58N/BAYCkwzs5fNrE9aI8xSnTrB4YfD2LFxRyIisq2U6/jN7AozKwOuAn4M7AH8BHg0jfFltdLSUN//+edxRyIislWqVT2vAy2AU919gLs/5e4b3b0MuD994WW3M84Ir089FW8cIiIVpZr4f+rut7n7vPIFZjYIwN1/kZbIckCXLnDooaruEZH6JdXEf10Vy66vy0ByVWkp/POf8GW9Gk5eRJKs2sRvZt8zs98Bbc1sZIVpDLAxIxFmudLS0I3z+PFxRyIiEtR0xv8lUAasBaZXmCYC/dIbWm446CAoKlJ1j4jUH9V2y+zu7wLvmtkj7q4z/FoqLYXbboMFC8IwjiIicaqpqueJaPZtM3uv8pSB+HJCaWnot+fpp+OORESk5oFYroheB6Y7kFxWVBQGcBk7Fi66KO5oRCTpqj3jd/evotlm7j634gR0Sn94ucEstOl/6SVYvDjuaEQk6VJtzvmEmV1rQZOopc/P0xlYrikthU2bQi+eIiJxSjXx9wLaA1OBtwitfY5OV1C5qLg4DNqi1j0iErdUE/8GYA3QBCgAPnV3DTOyE8zCWf/zz8M338QdjYgkWaqJ/y1C4j8COBY428yeTFtUOaq0FDZuhIkT445ERJIs1cR/vrvf5O4b3P0rdz+F8BCX7ISSkjBYi6p7RCROqSb+6WY22MxuAjCzDsDH6QsrN5VX90yaBMuWxR2NiCRVqon/90Af4Ozo/QrgvrRElOMGDYL161XdIyLxSblVj7tfSuizB3f/BmictqhyWK9e0L49PKk7JCISk5Rb9ZhZHuAAZtYGUKueWjALZ/3PPafqHhGJR6qJfyQwHtjTzO4AXgN+lraocpyqe0QkTqkOtv4IcA3had2vCEMw1rqywsxamdlYM5tpZjOSNmB7r16hdY+qe0QkDtV20mZmu1d4uxB4rOJn7v51Lff7W+BZdy81s8ZA01puJyuVt+65995Q3dOyZdwRiUiS1HTGP50wEMv0Kqay2uzQzFoC3wJGAbj7endfWpttZTNV94hIXGrqnbOTu3eOXitPnWu5z07AImC0mb1tZv9jZs0qr2RmF5pZmZmVLVq0qJa7qr/Kq3ueeKLmdUVE6lKqN3cxs9PN7Ddm9mszO3UX9tkQOAz4g7v3BFZRxWDu7v6gu5e4e0mbNm12YXf1U8WHuZYm7npHROKUUuI3s98DFwPvAx8AF5tZbR/gmgfMc/c3o/djCQeCxPnBD1TdIyKZl+oZ//FAP3cf7e6jgZOiZTvN3ecDn5tZ12jRCcBHtdlWtjvySLXuEZHMSzXxfwJ0qPC+fbSstn4MPBKN21tMQp8JUHWPiMQh1cTfHJhhZlPM7CXCGXoLM5toZjtdUeHu70T19z3c/dSoC4hEUnWPiGRaTYOtl7sprVEkWMXqnv/6r7ijEZEkqDHxR330jHD34zIQT+KU990zcmSo7mnVKu6IRCTX1VjV4+6bgM3Rg1eSBoMGwYYNqu4RkcxItapnJfC+mU0mtLsHwN0vT0tUCaPqHhHJpFQT/1PRJGmg6h4RyaRUe+d8GHgCeMPdHy6f0htasqi6R0QyJdUnd78PvAM8G70vrk0zTtmxI48MI3NpIHYRSbdU2/GPAI4ElkJohw/UtpM2qUL5w1zPPQfLl8cdjYjkspSHXnT3ygMFaujFOlZaGh7meuaZuCMRkVyWauL/0Mz+H5BnZl3M7HfA1DTGlUi9e8O++6q6R0TSK9XE/2OgCFgHPAosA65MV1BJ1aABnHEG/OMfsHJl3NGISK6qNvGbWYGZXQn8EvgM6OPuR7j7T919bUYiTJjSUli7Fv7+97gjEZFcVdMZ/8NACaEf/u8Bv0p7RAl39NGw996q7hGR9KnpAa5u7n4IgJmNAqalP6Rky8uD00+HMWNg9Wpomqhh6EUkE2o6499QPuPuG9Mci0RKS0PS/8c/4o5ERHJRTYn/UDNbHk0rgB7l82am1uZpcuyx0KaNqntEJD2qrepx97xMBSJbNWwIp50Gjz4Ka9ZAkyZxRyQiuSTV5pySYaWloUnnpElxRyIiuUaJv57q2xd2313VPSJS95T466lGjUJ1z8SJsG5d3NGISC5R4q/HSktDh22TJ8cdiYjkEiX+euz448OgLKruEZG6pMRfjzVuDKecAhMmhF47RUTqghJ/PVdaGoZjfOGFuCMRkVyhxF/Pffe7obrnscfijkREcoUSfz2Xnw9nnhnq+TUyl4jUBSX+LHDuueEJXt3kFZG6oMSfBXr1gq5dQ4+dIiK7Sok/C5iFs/5XX4XZs+OORkSynRJ/lhg8OBwA/vSnuCMRkWynxJ8l2rULLXwefhg2b447GhHJZrElfjPLM7O3zeyZuGLINueeC3Pnwssvxx2JiGSzOM/4rwBmxLj/rHPqqdCiRTjrFxGprVgSv5m1AwYA/xPH/rNVkyZb2/SvXBl3NCKSreI6478HuAbYYW21mV1oZmVmVrZo0aLMRVbPnXsurFqlNv0iUnsZT/xmNhBY6O7Tq1vP3R909xJ3L2nTpk2Goqv/+vSBLl3Upl9Eai+OM/6jgZPNbA7wF+B4M/tzDHFkJTMYOjTc4P3007ijEZFslPHE7+7Xu3s7d+8InAW86O6DMx1HNhsyRG36RaT21I4/C3XoACecoDb9IlI7sSZ+d5/i7gPjjCFbnXtuqOp59dW4IxGRbKMz/ix12mnQvDncfTe4xx2NiGQTJf4s1bQp3HBDGJbxl7+MOxoRySZK/Fns2mvhrLPg+uth4sS4oxGRbKHEn8XM4I9/hMMPh3POgfffjzsiEckGSvxZrkmTUN3TogV8//ugh5xFpCZK/Dlg333h6adhwQI4/XRYvz7uiESkPlPizxFHHBG6cXjtNbjkErX0EZEdaxh3AFJ3zjwTPvwQbrsNuneH4cPjjkhE6iOd8eeYESNCdc9VV8HkyXFHIyL1kRJ/jmnQIHTlUFQUrgA0OLuIVKbEn4MKC8PNXgijdmnQFhGpSIk/R3XuDE88AR99FPr10c1eESmnxJ/DvvMduOsuGDcO7rgj7mhEpL5Q4s9xw4fD4MFw003w17/GHY2I1AdK/DnODB58EA47LHTrMHNm3BGJSNyU+BOgSRMYPz68nnKKbvaKJJ0Sf0K0bx9u9s6aBdddF3c0IhInJf4E+fa34cor4b77YMqUuKMRkbgo8SfM7bfDAQfA+efDqlVxRyMicVDiT5imTUMf/p9+GkbwEpHkUeJPoGOPhcsug5Ej4ZVX4o5GRDJNiT+hfv7z8HTvsGGwenXc0YhIJinxJ1SzZjBqVOjE7ac/jTsaEckkJf4E69sXfvQjuOce+Oc/445GRDJFiT/hfvEL2G8/OO88WLEi7mhEJBOU+BOusDBU+XzyCXTtCqNHw6ZNcUclIumkxC8cfzxMnRrO/IcNg5ISePHFuKMSkXRR4hcAevcOyf8vf4FvvoETTgj9+nz8cdyRiUhdU+KXLczCcI0zZ8Kdd8JLL4VB2y+7DBYsiDs6EakrSvyynYICuPbaUO//wx/C/ffD/vvDzTfD8uVxRyciuyrjid/M2pvZS2b2kZl9aGZXZDoGSc2ee8Lvfx+GbzzpJLj11nAA+O1vYd26uKMTkdqK44x/I/ATd+8G9AYuNbNuMcQhKTrwwNCl87Rp0KNH6OHzoIPCAC+6AhDJPhlP/O7+lbv/K5pfAcwA2mY6Dtl5RxwBzz8Pzz0Hu+8OF10Ee+8dhnZ8/nnYvDnuCEUkFbHW8ZtZR6An8GYVn11oZmVmVrZo0aJMhyY7YAYnnghlZfDGGzB0KDzzDHz3u9CxY+j+YdasuKMUkeqYu8ezY7NC4GXgDnd/qrp1S0pKvKysLDOByU5buxYmToQxY8LVwObNoQfQYcOgtDQ8JCYimWdm0929pPLyWM74zawRMA54pKakL/VfQQH84Afw97/DZ5+FpqALFoRuIPbZJwz68s9/QkznGCJSSRytegwYBcxw999kev+SXm3bhqagM2fCa6+FA8Ljj8Mxx4Qbwr/+NSxeHHeUIskWxxn/0cAQ4HgzeyeaToohDkkjMzj66NAP0Pz5oQ+gNm3gqqvCwWHwYHj1VV0FiMQhtjr+naE6/tzxwQfwwAPwv/8Ly5ZBt25wwQXQqVM4WEB4LZ86dICDD4ZGjeKNWyQb7aiOX4lfYrFqVagCeuCB8HxAdfLzw/MDhx8Ohx0WpkMOgcaNMxOrSLZS4pd6a/bsrQ+CuW+dNm8On/3rX1unpUvDes2bw8knh1ZD/fpBkybbb3fePBg/HsaNg7feCp3OXXklHHlk5somEiclfsl67vDppzB9emg2On48fP11aC46cGA4CBx0EPztbyHZl19JFBWFq4Wnnw4HmD594Ior4PTTVYUkuU2JX3LOhg0wZQqMHQtPPbVta6GSkpDYTz89DDADYYSxMWNCX0OzZ0O7dnDppdCrF+yxB7RuHab8/LD+5s0wZw68/36Y3nsvvJqFq40zzgj7Kb83IVLfKPFLTtu4EV55JfQo2q9fGFRmRzZvDlcF99xT9YAzzZuHA8DixbBy5dblnTuHewurVoUuqzdtCjefTz89HASOOgoaVGgn5x4OTps2hWcddICQTFPiF6nCf/4Dc+fCkiUh0Ze/Ll4MrVqFm8qHHBKqi5o33/q9r78OTyuPGweTJsH69dC0aUj8GzaEA1HFISwbNQr9G1WeWrQIVVXNm2/7uttuofnrnnuGq5GGDXeuXJs3hyuclSvDTfAmTcKUl7d1nbVrYcaMrVc05VOzZuFgNmhQuJGuA1b2UuIXSZPly8MVxBtvhMTaqFFI1I0ahalBg7DO119vPy1fHhJ0TeMc7757OAi0bLltIi6f37QpbGf58q3brEp+fjgAFBTAokVb95ufH5rWHnJIeO7ihRfCZ506hXsnpaWhk76qDgLr1oUqsdmzt53mzg0Hsb33Dk9w77331vk2bbZWrbVsue2VUkXusGZNmG/atPrfaNOmUPX32GPw17+G3+vYY7dO7dpV//2dsWZNeDq9oCCcIBQU1G47GzeG7axZE+Kr7XZ2RIlfpJ5yD1cM5WfoK1aE4S8XLgzTokVb5wUK5kQAAAejSURBVCt2g13xv65ZuHqoOLVsGc7eN26E1avDtGbN1vl99gmJvkcPOOCAba8qliyBCRPC/ZPJk8M2CgtDgt60adupcq+szZqFcRv22y/sZ/58+OqrcKCrSoMG4cDWunW4Olm1KkwrV4bvu4fydekSbtKXTz17hnK+/noYMvSJJ0ISLb/Z//XXYTjR8uq6jh3DAaBr13Bl1aLF1tcWLcJBuvy3qTgtWwaffx66I/nsszC/cOG2ZWjcOBwAWrYMU/Pm4UDVrNm2r6tXwxdfbJ3mz9/299tzz1B92KEDtG8fXocMCQfK2lDiF5Fa+eabcBB4++2QpPPytr7m5YWrhY4dQ7Lff/+QvHZ0ZbBgQTgIlFerVZ7Wrw+Ju1mzMJXPr10b9j99emimW2633UJ8+fkh2Z99dhg0qLx578aN8O67ofuQV18Nr7UZRrRZs3AgK0/KHTqEA+e6deHAsHTptq8rV4aD1+rV274WFIQn1ytPBQWhXBUPMHPnhu988kn4XWtDiV9EcsLCheGZjunTQ5XScceFZzRatEjt++VXV5Wrxsrv01SeCgvD2Xym73W4hwNJixbb3pvZGUr8IiIJU6+6ZRYRkfgo8YuIJIwSv4hIwijxi4gkjBK/iEjCKPGLiCSMEr+ISMIo8YuIJExWPMBlZouAuTWstgewuIZ1cpHKnSwqd7Lsarn3c/ftevrJisSfCjMrq+oJtVyncieLyp0s6Sq3qnpERBJGiV9EJGFyKfE/GHcAMVG5k0XlTpa0lDtn6vhFRCQ1uXTGLyIiKVDiFxFJmKxP/GbW38w+NrNPzOy6uOOpa2b2RzNbaGYfVFi2u5lNNrNZ0etu0XIzs5HRb/GemR0WX+S1Z2btzewlM/vIzD40syui5TldbgAzKzCzaWb2blT2W6LlnczszaiMj5tZ42h5fvT+k+jzjnHGvyvMLM/M3jazZ6L3OV9mADObY2bvm9k7ZlYWLUvr33pWJ34zywPuA74HdAPONrNu8UZV58YA/Sstuw54wd27AC9E7yH8Dl2i6ULgDxmKsa5tBH7i7t2A3sCl0b9rrpcbYB1wvLsfChQD/c2sN/AL4G53PwD4Bjg/Wv984Jto+d3RetnqCmBGhfdJKHO549y9uEKb/fT+rbt71k5AH+C5Cu+vB66PO640lLMj8EGF9x8D+0Tz+wAfR/MPAGdXtV42T8AE4LsJLHdT4F9AL8LTmw2j5Vv+7oHngD7RfMNoPYs79lqUtV2U4I4HngEs18tcoexzgD0qLUvr33pWn/EDbYHPK7yfFy3LdXu5+1fR/Hxgr2g+536P6DK+J/AmCSl3VOXxDrAQmAzMBpa6+8ZolYrl21L26PNlQOvMRlwn7gGuATZH71uT+2Uu58AkM5tuZhdGy9L6t96wtpFK/eDubmY52SbXzAqBccCV7r7czLZ8lsvldvdNQLGZtQLGAwfFHFJamdlAYKG7TzezvnHHE4Nj3P0LM9sTmGxmMyt+mI6/9Ww/4/8CaF/hfbtoWa5bYGb7AESvC6PlOfN7mFkjQtJ/xN2fihbnfLkrcvelwEuEao5WZlZ+olaxfFvKHn3eEliS4VB31dHAyWY2B/gLobrnt+R2mbdw9y+i14WEA/2RpPlvPdsT/1tAl+juf2PgLGBizDFlwkRgaDQ/lFAHXr78v6I7/72BZRUuF7OGhVP7UcAMd/9NhY9yutwAZtYmOtPHzJoQ7m3MIBwASqPVKpe9/DcpBV70qPI3W7j79e7ezt07Ev4Pv+ju55DDZS5nZs3MrHn5PHAi8AHp/luP+8ZGHdwYOQn4N6Ee9L/jjicN5XsM+ArYQKjPO59Qn/kCMAt4Htg9WtcIrZxmA+8DJXHHX8syH0Oo93wPeCeaTsr1ckdl6QG8HZX9A+CmaHlnYBrwCfAkkB8tL4jefxJ93jnuMuxi+fsCzySlzFEZ342mD8tzWLr/1tVlg4hIwmR7VY+IiOwkJX4RkYRR4hcRSRglfhGRhFHiFxFJGCV+SSwz2xT1iFg+1VnvrmbW0Sr0qCpSn6jLBkmyNe5eHHcQIpmmM36RSqL+0X8Z9ZE+zcwOiJZ3NLMXo37QXzCzDtHyvcxsfNSH/rtmdlS0qTwzeyjqV39S9CQuZna5hbEG3jOzv8RUTEkwJX5JsiaVqnrOrPDZMnc/BLiX0HMkwO+Ah929B/AIMDJaPhJ42UMf+ocRnsCE0Gf6fe5eBCwFzoiWXwf0jLZzcboKJ7IjenJXEsvMVrp7YRXL5xAGQ/lP1FncfHdvbWaLCX2fb4iWf+Xue5jZIqCdu6+rsI2OwGQPA2lgZtcCjdz9djN7FlgJPA087e4r01xUkW3ojF+kar6D+Z2xrsL8JrbeUxtA6G/lMOCtCj1QimSEEr9I1c6s8Pp6ND+V0HskwDnAq9H8C8AlsGUQlZY72qiZNQDau/tLwLWELoW3u+oQSSedaUiSNYlGuir3rLuXN+nczczeI5y1nx0t+zEw2syuBhYB50XLrwAeNLPzCWf2lxB6VK1KHvDn6OBgwEgP/e6LZIzq+EUqier4S9x9cdyxiKSDqnpERBJGZ/wiIgmjM34RkYRR4hcRSRglfhGRhFHiFxFJGCV+EZGE+T8F0PQVCdsSswAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_perplexity(perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.6871, -0.3593,  0.4704, -0.6224]],\n",
      "\n",
      "        [[-0.3548, -0.5317,  0.3520, -0.8332]],\n",
      "\n",
      "        [[ 0.5139, -0.5306, -0.1322, -0.7962]],\n",
      "\n",
      "        [[ 0.1453, -0.1416,  0.4736, -0.6271]],\n",
      "\n",
      "        [[ 0.0465, -0.4846,  0.1393, -0.8029]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Ex1\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(42);\n",
    "\n",
    "rnn = nn.RNN(2, 4)\n",
    "input = torch.randn(5, 1, 2)\n",
    "h0 = torch.randn(1, 1, 4)\n",
    "output, hn = rnn(input, h0)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traveller the this the this t\n",
      "traveller a sto and so said t\n",
      "traveller a say so of sald th\n",
      "traveller murif so whit so di\n",
      "traveller a fore all al soles\n",
      "traveller the said the said t\n",
      "traveller the prover a movess\n",
      "traveller the said the time t\n",
      "traveller the time traveller \n",
      "traveller the medical so we s\n",
      "traveller the time traveller \n",
      "travellery which a line a lat\n",
      "traveller the meriment of spa\n",
      "traveller a line a line a lin\n",
      "traveller the time traveller \n",
      "traveller the procerine trave\n",
      "traveller so man and why he s\n",
      "traveller so expere so explon\n",
      "traveller the time traveller \n",
      "traveller the time traveller \n",
      "perplexity 5.1, device cuda:0\n",
      "traveller the time traveller \n"
     ]
    }
   ],
   "source": [
    "#Ex2\n",
    "\n",
    "import hashlib\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import collections\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "\n",
    "torch.manual_seed(42);\n",
    "\n",
    "def download(url, cache_dir=os.path.join('..', 'data')):\n",
    "    \"\"\"Download a file, return the local filename.\"\"\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
    "    if os.path.exists(fname):\n",
    "        with open(fname, 'rb') as f:\n",
    "            while True:\n",
    "                data = f.read(1048576)\n",
    "                if not data:\n",
    "                    break\n",
    "        return fname\n",
    "    print(f'Downloading {fname} from {url}...')\n",
    "    r = requests.get(url, stream=True, verify=True)\n",
    "    with open(fname, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    return fname\n",
    "\n",
    "def read_time_machine():\n",
    "    \"\"\"Load the time machine dataset into a list of text lines.\"\"\"\n",
    "    with open(download('http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
    "\n",
    "def tokenize(lines, token='word'):\n",
    "    \"\"\"Split text lines into word or character tokens.\"\"\"\n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('ERROR: unknown token type: ' + token)\n",
    "        \n",
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        # Sort according to frequencies\n",
    "        counter = count_corpus(tokens)\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                   reverse=True)\n",
    "        # The index for the unknown token is 0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # Index for the unknown token\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):  # Token frequencies\n",
    "        return self._token_freqs\n",
    "\n",
    "def count_corpus(tokens):\n",
    "    \"\"\"Count token frequencies.\"\"\"\n",
    "    # Here `tokens` is a 1D list or 2D list\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # Flatten a list of token lists into a list of tokens\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)\n",
    "\n",
    "def load_corpus_time_machine(max_tokens=-1):\n",
    "    \"\"\"Return token indices and the vocabulary of the time machine dataset.\"\"\"\n",
    "    lines = read_time_machine()\n",
    "    tokens = tokenize(lines, 'char')\n",
    "    vocab = Vocab(tokens)\n",
    "    # Since each text line in the time machine dataset is not necessarily a\n",
    "    # sentence or a paragraph, flatten all the text lines into a single list\n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    return corpus, vocab\n",
    "\n",
    "def seq_data_iter_sequential(corpus, batch_size, num_steps):\n",
    "    \"\"\"Generate a mini-batch of subsequences using sequential partitioning.\"\"\"\n",
    "    # Start with a random offset to partition a sequence\n",
    "    offset = random.randint(0, num_steps)\n",
    "    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
    "    Xs = torch.tensor(corpus[offset: offset + num_tokens])\n",
    "    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n",
    "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
    "    num_batches = Xs.shape[1] // num_steps\n",
    "    for i in range(0, num_steps * num_batches, num_steps):\n",
    "        X = Xs[:, i: i + num_steps]\n",
    "        Y = Ys[:, i: i + num_steps]\n",
    "        yield X, Y\n",
    "        \n",
    "class SeqDataLoader:\n",
    "    \"\"\"An iterator to load sequence data.\"\"\"\n",
    "    def __init__(self, batch_size, num_steps, max_tokens):\n",
    "        self.corpus, self.vocab = load_corpus_time_machine(max_tokens)\n",
    "        self.batch_size, self.num_steps = batch_size, num_steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        return seq_data_iter_sequential(self.corpus, self.batch_size, self.num_steps)\n",
    "    \n",
    "def load_data_time_machine(batch_size, num_steps, max_tokens=10000):\n",
    "    \"\"\"Return the iterator and the vocabulary of the time machine dataset.\"\"\"\n",
    "    data_iter = SeqDataLoader(\n",
    "        batch_size, num_steps, max_tokens)\n",
    "    return data_iter, data_iter.vocab\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"The RNN model.\"\"\"\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size\n",
    "        self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
    "        X = X.to(torch.float32)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        # The fully connected layer will first change the shape of `Y` to\n",
    "        # (`num_steps` * `batch_size`, `num_hiddens`). Its output shape is\n",
    "        # (`num_steps` * `batch_size`, `vocab_size`).\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            # `nn.GRU` takes a tensor as hidden state\n",
    "            return  torch.zeros((self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens),\n",
    "                                 device=device)\n",
    "        else:\n",
    "            # `nn.LSTM` takes a tuple of hidden states\n",
    "            return (torch.zeros((\n",
    "                self.rnn.num_layers,\n",
    "                batch_size, self.num_hiddens), device=device),\n",
    "                    torch.zeros((\n",
    "                        self.rnn.num_layers,\n",
    "                        batch_size, self.num_hiddens), device=device))\n",
    "        \n",
    "def train_epoch(net, train_iter, loss, optimizer, device):\n",
    "    \"\"\"Train a net within one epoch.\"\"\"\n",
    "    state = None\n",
    "    # Sum of training loss, no. of tokens\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    for X, Y in train_iter:\n",
    "        if state is None:\n",
    "            # Initialize `state` when it is the first iteration\n",
    "            state = net.begin_state(batch_size=X.shape[0], device=device)\n",
    "        else:\n",
    "            if not isinstance(state, tuple):\n",
    "                # `state` is a tensor for `nn.GRU`\n",
    "                state.detach_()\n",
    "            else:\n",
    "                # `state` is a tuple of tensors for `nn.LSTM`\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "        y = Y.T.reshape(-1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_hat, state = net(X, state)\n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        grad_clipping(net, 1)\n",
    "        optimizer.step()\n",
    "        total_loss += float(l * y.numel())\n",
    "        total_tokens += y.numel()\n",
    "    return math.exp(total_loss / total_tokens)\n",
    "\n",
    "def train(net, train_iter, vocab, lr, num_epochs, device):\n",
    "    \"\"\"Train a model.\"\"\"\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    perplexities = []\n",
    "    # Initialize\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr)\n",
    "    # Train and predict\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl = train_epoch(\n",
    "            net, train_iter, loss, optimizer, device)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(predict('traveller', 20, net, vocab, device))\n",
    "            perplexities.append(ppl)\n",
    "    print(f'perplexity {ppl:.1f}, device {str(device)}')\n",
    "    #print(predict('time traveller', 50, net, vocab, device))\n",
    "    print(predict('traveller', 20, net, vocab, device))\n",
    "\n",
    "    return perplexities\n",
    "\n",
    "def grad_clipping(net, theta):\n",
    "    \"\"\"Clip the gradient.\"\"\"\n",
    "    params = [p for p in net.parameters() if p.requires_grad]\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm\n",
    "\n",
    "def predict(prefix, num_preds, net, vocab, device):\n",
    "    \"\"\"Generate new characters following the `prefix`.\"\"\"\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
    "    for y in prefix[1:]:  # Warm-up period\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds):  # Predict `num_preds` steps\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])\n",
    "            \n",
    "def try_gpu(i=0):\n",
    "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "batch_size, num_steps = 30, 10\n",
    "train_iter, vocab = load_data_time_machine(batch_size, num_steps)\n",
    "\n",
    "num_hiddens = 32\n",
    "rnn_layer = nn.RNN(len(vocab), num_hiddens, 2)\n",
    "\n",
    "device = try_gpu()\n",
    "net = RNNModel(rnn_layer, vocab_size=len(vocab))\n",
    "net = net.to(device)\n",
    "\n",
    "num_epochs, lr = 200, 1.5\n",
    "perplexities = train(net, train_iter, vocab, lr, num_epochs, device) #1 min\n",
    "#print(predict('traveller', 20, net, vocab, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
